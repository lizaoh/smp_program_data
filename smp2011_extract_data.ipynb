{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM5xY1ANSPPtLFwfQ5B+742",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizaoh/smp_program_data/blob/main/smp2011_extract_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top of Script"
      ],
      "metadata": {
        "id": "KBXc6uieLC6Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW0MLlXyhw5G",
        "outputId": "4cad9d83-8a2c-4d95-cb11-35f545eb98df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "!pip install pymupdf-layout\n",
        "!pip install pymupdf4llm\n",
        "# !pip install rapidfuzz\n",
        "import glob\n",
        "import os\n",
        "import pathlib\n",
        "import pymupdf\n",
        "import pymupdf.layout\n",
        "import pymupdf4llm\n",
        "import re\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "# from rapidfuzz import process, fuzz"
      ],
      "metadata": {
        "id": "iwaO2sqpib2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b0a91ca-c736-43d4-c26e-5169f8f9e036"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.7\n",
            "Collecting pymupdf-layout\n",
            "  Downloading pymupdf_layout-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting PyMuPDF==1.26.6 (from pymupdf-layout)\n",
            "  Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from pymupdf-layout) (6.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pymupdf-layout) (2.0.2)\n",
            "Collecting onnxruntime (from pymupdf-layout)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pymupdf-layout) (3.6.1)\n",
            "Collecting coloredlogs (from onnxruntime->pymupdf-layout)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (1.14.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->pymupdf-layout)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime->pymupdf-layout) (1.3.0)\n",
            "Downloading pymupdf_layout-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m147.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m143.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF, humanfriendly, coloredlogs, onnxruntime, pymupdf-layout\n",
            "  Attempting uninstall: PyMuPDF\n",
            "    Found existing installation: PyMuPDF 1.26.7\n",
            "    Uninstalling PyMuPDF-1.26.7:\n",
            "      Successfully uninstalled PyMuPDF-1.26.7\n",
            "Successfully installed PyMuPDF-1.26.6 coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.2 pymupdf-layout-1.26.6\n",
            "Collecting pymupdf4llm\n",
            "  Downloading pymupdf4llm-0.2.7-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pymupdf>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from pymupdf4llm) (1.26.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from pymupdf4llm) (0.9.0)\n",
            "Downloading pymupdf4llm-0.2.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf4llm\n",
            "Successfully installed pymupdf4llm-0.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdfs_path = '/content/drive/MyDrive/math_psych_work/Conference Programs/'"
      ],
      "metadata": {
        "id": "9TqNj0OYhzgb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions\n",
        "Created with help from GPT 5.2, but some are my own code just turned into a function."
      ],
      "metadata": {
        "id": "cK6Itu79JGuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AFFILIATION_KEYWORDS = [\n",
        "    \"University\", \"College\", \"Department\", \"Center\", \"Institute\",\n",
        "    \"Laboratory\", \"School\", \"Hospital\", \"UC\", \"Centre\", \"Research\",\n",
        "    \"Corporation\", \"Defence\", \"Université\", \"Universite\", \"Universiy\"\n",
        "]\n",
        "AFFILIATION_KEYWORDS = re.compile(r'\\b(' + '|'.join(AFFILIATION_KEYWORDS) + r')\\b',\n",
        "                                  re.I)\n",
        "\n",
        "def looks_like_affiliation(chunk):\n",
        "    return bool(AFFILIATION_KEYWORDS.search(chunk))"
      ],
      "metadata": {
        "id": "t0fWgJSqXKPJ"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_whitespace(s: str) -> str:\n",
        "    return \" \".join(s.replace(\"\\n\", \"\").split())"
      ],
      "metadata": {
        "id": "W6PGuwc6e3jZ"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_affiliations(entry: str) -> str:\n",
        "    return re.sub(\n",
        "        r'(University of California)\\s*,?\\s*'\n",
        "        r'(Irvine|Davis|Berkeley|Los Angeles|San Diego|Santa Barbara|Santa Cruz|Riverside|Merced)',\n",
        "        r'\\1, \\2',\n",
        "        entry\n",
        "    )"
      ],
      "metadata": {
        "id": "ilIOFaZ8ArV9"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_authors_affiliations(entry: str) -> tuple[str, str]:\n",
        "    entry = normalize_whitespace(entry)\n",
        "    entry = normalize_affiliations(entry)\n",
        "\n",
        "    # split only on AND / & / commas WITH SPACES\n",
        "    tokens = re.split(r'\\s+(?:and|&)\\s+|,\\s+(?=[A-Z])', entry)\n",
        "\n",
        "    authors = []\n",
        "    affiliations = []\n",
        "\n",
        "    for token in tokens:\n",
        "        token = token.strip()\n",
        "\n",
        "        if looks_like_affiliation(token):\n",
        "            affiliations.append(token)\n",
        "        else:\n",
        "            authors.append(token)\n",
        "\n",
        "    return (\n",
        "        \", \".join(authors),\n",
        "        \"; \".join(affiliations)\n",
        "    )"
      ],
      "metadata": {
        "id": "1DuX4taa_qAo"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if not text:\n",
        "        return text\n",
        "\n",
        "    text = re.sub(r' \\n\\n\\d{1,3} \\n\\n', ' ', text)\n",
        "    text = re.sub(r'\\s*\\n\\s*', ' ', text)    # Replace newlines with spaces\n",
        "\n",
        "    text = re.sub(r'-\\s+', '', text)         # Get rid of \"- \"; will fix actual\n",
        "                                             # hyphenated words manually\n",
        "\n",
        "    text = re.sub(r'\\s{2}', ' ', text)       # Collapse two adjacent spaces into one\n",
        "\n",
        "    text = re.sub(r'\\.\\s*##.*$', '.', text,\\\n",
        "                  flags=re.DOTALL)           # Gets rid of extraneous text after\n",
        "                                             # last sentence\n",
        "    text = text.strip()\n",
        "    text = fix_ligatures(text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "aq4f3JE9IgHD"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LIGATURE_MAP = {\n",
        "    \"ﬁ\": \"fi\",\n",
        "    \"ﬂ\": \"fl\",\n",
        "    \"ﬃ\": \"ffi\",\n",
        "    \"ﬄ\": \"ffl\",\n",
        "    \"ﬀ\": \"ff\",\n",
        "    \"ﬅ\": \"ft\",\n",
        "    \"ﬆ\": \"st\",\n",
        "    \"Æ\": 'ffi'\n",
        "}\n",
        "\n",
        "def fix_ligatures(text):\n",
        "    # Replace known ligatures\n",
        "    for bad, good in LIGATURE_MAP.items():\n",
        "        text = text.replace(bad, good)\n",
        "\n",
        "    # Replace any private-use ligature (common in PDFs)\n",
        "    cleaned_chars = []\n",
        "    for ch in text:\n",
        "        name = unicodedata.name(ch, \"\")\n",
        "        if \"LIGATURE\" in name.upper():\n",
        "            # Try to break it apart: remove spaces and lowercase\n",
        "            base = name.split(\"LIGATURE\")[-1]\n",
        "            base = base.replace(\" \", \"\").lower()\n",
        "            cleaned_chars.append(base)\n",
        "        else:\n",
        "            cleaned_chars.append(ch)\n",
        "\n",
        "    return \"\".join(cleaned_chars)"
      ],
      "metadata": {
        "id": "WLDsGBzsFnSz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Program\n",
        "\n",
        "Extracting to markdown doesn't show when text is bold or italic even though it is in the pdf, so just extracting as text.\n",
        "\n",
        "145 entries total (3 plenary, 22 symposium talks, 101 normal talks, 19 posters)."
      ],
      "metadata": {
        "id": "EzyoCbvJK6js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grab text from the pdf"
      ],
      "metadata": {
        "id": "NY0NZXAh696-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "year = '2011'\n",
        "program = pymupdf.open(pdfs_path + f'smp{year}_program.pdf')"
      ],
      "metadata": {
        "id": "-iaeTkOjqA3-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting as markdown to more easily get title, authors, and affiliations\n",
        "# Title is bold, affiliations are italic, etc.\n",
        "program_text = pymupdf4llm.to_text(program)"
      ],
      "metadata": {
        "id": "GB3Wc1MBHp8X"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "program_text[14500:18000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "JWzEUEew4_1b",
        "outputId": "08a3e042-f02f-4a86-9ed8-0a30c948c5cb"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'laces on the Boston Avenue side of the campus. Also there are several good restaurants in the Davis Square area, which is a ten-minute walk away from campus on College Avenue. \\n\\n6 \\n\\nAbstracts For Plenary Talks \\n\\n(Plenary abstracts organized by day) \\n\\n(Plenary) \\n\\nSaturday, 1:45 Plenary Changing the State of the Observer in Visual Processing: Perceptual Learning and Attention. Barbara Anne Dosher, University of California, Irvine. Processing of visual inputs is fundamental to how we interact with the world. The quality of performance depends in complex ways on properties of the stimulus as well as limitations in the observers system. It may also depend on the changing state of the observer, including changing states of perceptual learning, attention, adaptation, and others. This talk focuses on the phenomenon of perceptual learning, which improves perceptual task performance in several distinctive ways. External noise tests and noisy ideal observer models characterize the effects of perceptual learning (or attention) on an observer as a single system. Across a range of tasks, two independent mechanisms can be seen: tuning of the task relevant perceptual template to improve filtering (external noise exclusion) and enhancing the stimulus (reducing absolute threshold). Several technical properties eliminate classes of prior models. Performance signatures for observer models then generate constraints on more detailed mod- \\n\\nels. At the next level, implementations of multi-channel sensory representations suggest that, for perceptual learning, both mechanisms may reflect re-weighting of information from early sensory codes. Principles of reweighting through augmented Hebbian learning provide an account of a range of phenomena in perceptual learning, including the roles of feedback, training accuracy, task precision, and transfer to related but different tasks. The framework provides a systematic framework to consider the joint effects of stimuli and tasks in perceptual learning. \\n\\n(Plenary) Sunday, 1:45 \\n\\nPlenary \\n\\nTraining Hierarchical Systems for Visual Perception. Yann LeCun, Courant Institute, New York University. \\n\\nOver the last few years, a surprising convergence has occurred between research in computational visual neuroscience, visual psychophysics, object recognition with artificial vision systems, the branch of applied mathematics concerned with sparse representations of high-dimensional data, and a new sub-field of machine learning dubbed “deep learning”. \\n\\nThe best artificial vision systems bear an increasingly strong resemblance with their natural counterpart, exhibit some of the \\n\\n7 \\n\\nqualities and deficiencies of the human visual system observed by psychologists. Like their natural counterparts, these artificial systems relies heavily on learning (unsupervised and supervised), and use building blocks and architectural concepts borrowed from neuroscience. The architectures are multi-level stacks analogous to the V1-V2V4-IT hierarchy, that includes modules such as highly-non-linear local feature detectors with sparse activations, divisive normalization modules that create competition between nearby feature detectors, and spatial pooling modules la complex cells to build invariance. \\n\\nTraining such multi-stage systems to produce a hierarchy of increasingly global and invariant representations requires new “deep learning” algorithms. A system will be described that that combines a phase of unsupervised learning based on sparse sparse'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BEFORE_TITLE_PHRASES = ['Plenary', 'Cohen', 'Distler',\n",
        "                        r'East Sophia Gordon(?: Hall)? Bldg\\.',\n",
        "                        'Alumni', r'Board [A-E] #[1-4]']\n",
        "before_title_re = re.compile(\n",
        "    r'(?:' + '|'.join(BEFORE_TITLE_PHRASES) + r')\\s+'\n",
        ")\n",
        "\n",
        "poster_split = re.split(r'(?:Saturday|Sunday|Monday), \\d{1,2}:\\d{2}-\\d{1,2}:\\d{2}\\s*',\\\n",
        "                         program_text)\n",
        "talks_split = re.split(r'(?:Saturday|Sunday|Monday), \\d{1,2}:\\d{2}\\s*',\\\n",
        "                           poster_split[0])\n",
        "all_entries = talks_split[1:] + poster_split[1:]\n",
        "\n",
        "split_entries = [\n",
        "    part.strip()\n",
        "    for entry in all_entries\n",
        "    for part in before_title_re.split(entry)\n",
        "    if part.strip()\n",
        "][:-8]\n",
        "split_entries[-1] = re.split(r' \\n\\nAuthor Index', split_entries[-1], 1)[0]"
      ],
      "metadata": {
        "id": "lI3nmfGry7ke"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_entries = []\n",
        "\n",
        "for entry in split_entries:\n",
        "  if '<==\\n\\n' in entry:\n",
        "    fixed_entries.extend(entry.split('<==\\n\\n'))\n",
        "  else:\n",
        "    fixed_entries.append(entry)"
      ],
      "metadata": {
        "id": "UTUcRVticTk-"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_entries = [entry for entry in fixed_entries if len(entry.strip()) > 750]"
      ],
      "metadata": {
        "id": "lYCDK-UewmEu"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_entries[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvaMJyHO3E_9",
        "outputId": "0c0373eb-eef4-4c31-a20b-7c4c85a25d15"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Changing the State of the Observer in Visual Processing: Perceptual Learning and Attention. Barbara Anne Dosher, University of California, Irvine. Processing of visual inputs is fundamental to how we interact with the world. The quality of performance depends in complex ways on properties of the stimulus as well as limitations in the observers system. It may also depend on the changing state of the observer, including changing states of perceptual learning, attention, adaptation, and others. This talk focuses on the phenomenon of perceptual learning, which improves perceptual task performance in several distinctive ways. External noise tests and noisy ideal observer models characterize the effects of perceptual learning (or attention) on an observer as a single system. Across a range of tasks, two independent mechanisms can be seen: tuning of the task relevant perceptual template to improve filtering (external noise exclusion) and enhancing the stimulus (reducing absolute threshold). Several technical properties eliminate classes of prior models. Performance signatures for observer models then generate constraints on more detailed mod- \\n\\nels. At the next level, implementations of multi-channel sensory representations suggest that, for perceptual learning, both mechanisms may reflect re-weighting of information from early sensory codes. Principles of reweighting through augmented Hebbian learning provide an account of a range of phenomena in perceptual learning, including the roles of feedback, training accuracy, task precision, and transfer to related but different tasks. The framework provides a systematic framework to consider the joint effects of stimuli and tasks in perceptual learning. \\n\\n(Plenary)',\n",
              " 'Training Hierarchical Systems for Visual Perception. Yann LeCun, Courant Institute, New York University. \\n\\nOver the last few years, a surprising convergence has occurred between research in computational visual neuroscience, visual psychophysics, object recognition with artificial vision systems, the branch of applied mathematics concerned with sparse representations of high-dimensional data, and a new sub-field of machine learning dubbed “deep learning”. \\n\\nThe best artificial vision systems bear an increasingly strong resemblance with their natural counterpart, exhibit some of the \\n\\n7 \\n\\nqualities and deficiencies of the human visual system observed by psychologists. Like their natural counterparts, these artificial systems relies heavily on learning (unsupervised and supervised), and use building blocks and architectural concepts borrowed from neuroscience. The architectures are multi-level stacks analogous to the V1-V2V4-IT hierarchy, that includes modules such as highly-non-linear local feature detectors with sparse activations, divisive normalization modules that create competition between nearby feature detectors, and spatial pooling modules la complex cells to build invariance. \\n\\nTraining such multi-stage systems to produce a hierarchy of increasingly global and invariant representations requires new “deep learning” algorithms. A system will be described that that combines a phase of unsupervised learning based on sparse sparse coding and sparse dictionary learning, followed by a phase of supervised finetuning. Examples of such biologicallyinspired vision systems will be described and demonstrated live, with applications to object recognition in natural images, object detection, and obstacle avoidance for mobile robots. \\n\\n(Plenary)']"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_entries = []\n",
        "\n",
        "for e, entry in enumerate(filtered_entries):\n",
        "  no_trailing_junk = entry.split(\".\")[:-1]\n",
        "  cleaned_entry = clean_text(\".\".join(no_trailing_junk))\n",
        "\n",
        "  title, rest_of_entry = cleaned_entry.split('.', 1)\n",
        "  auth_and_aff, abstract = re.split(r'(?<!\\b[A-Z])\\.\\s+(?=[A-Z])', rest_of_entry, 1)\n",
        "\n",
        "  authors, affiliations = split_authors_affiliations(auth_and_aff)\n",
        "  cleaned_abstract = clean_text(abstract)\n",
        "\n",
        "  parsed_entries.append({\n",
        "    'year': year,\n",
        "    'author(s)': authors,\n",
        "    'affiliation(s)': affiliations,\n",
        "    'title': title,\n",
        "    'type': '',\n",
        "    'abstract': cleaned_abstract + '.'  # add back last period\n",
        "  })"
      ],
      "metadata": {
        "id": "fHIheEMK3K8C"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_entries[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd2bOgcs3K0R",
        "outputId": "685b4893-4eee-40ca-a322-5040f0f1ea46"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'year': '2011',\n",
              "  'author(s)': 'Barbara Anne Dosher, Irvine',\n",
              "  'affiliation(s)': 'University of California',\n",
              "  'title': 'Changing the State of the Observer in Visual Processing: Perceptual Learning and Attention',\n",
              "  'type': '',\n",
              "  'abstract': 'Processing of visual inputs is fundamental to how we interact with the world. The quality of performance depends in complex ways on properties of the stimulus as well as limitations in the observers system. It may also depend on the changing state of the observer, including changing states of perceptual learning, attention, adaptation, and others. This talk focuses on the phenomenon of perceptual learning, which improves perceptual task performance in several distinctive ways. External noise tests and noisy ideal observer models characterize the effects of perceptual learning (or attention) on an observer as a single system. Across a range of tasks, two independent mechanisms can be seen: tuning of the task relevant perceptual template to improve filtering (external noise exclusion) and enhancing the stimulus (reducing absolute threshold). Several technical properties eliminate classes of prior models. Performance signatures for observer models then generate constraints on more detailed models. At the next level, implementations of multi-channel sensory representations suggest that, for perceptual learning, both mechanisms may reflect re-weighting of information from early sensory codes. Principles of reweighting through augmented Hebbian learning provide an account of a range of phenomena in perceptual learning, including the roles of feedback, training accuracy, task precision, and transfer to related but different tasks. The framework provides a systematic framework to consider the joint effects of stimuli and tasks in perceptual learning.'},\n",
              " {'year': '2011',\n",
              "  'author(s)': 'Yann LeCun',\n",
              "  'affiliation(s)': 'Courant Institute; New York University',\n",
              "  'title': 'Training Hierarchical Systems for Visual Perception',\n",
              "  'type': '',\n",
              "  'abstract': 'Over the last few years, a surprising convergence has occurred between research in computational visual neuroscience, visual psychophysics, object recognition with artificial vision systems, the branch of applied mathematics concerned with sparse representations of high-dimensional data, and a new sub-field of machine learning dubbed “deep learning”. The best artificial vision systems bear an increasingly strong resemblance with their natural counterpart, exhibit some of the qualities and deficiencies of the human visual system observed by psychologists. Like their natural counterparts, these artificial systems relies heavily on learning (unsupervised and supervised), and use building blocks and architectural concepts borrowed from neuroscience. The architectures are multi-level stacks analogous to the V1-V2V4-IT hierarchy, that includes modules such as highly-non-linear local feature detectors with sparse activations, divisive normalization modules that create competition between nearby feature detectors, and spatial pooling modules la complex cells to build invariance. Training such multi-stage systems to produce a hierarchy of increasingly global and invariant representations requires new “deep learning” algorithms. A system will be described that that combines a phase of unsupervised learning based on sparse sparse coding and sparse dictionary learning, followed by a phase of supervised finetuning. Examples of such biologicallyinspired vision systems will be described and demonstrated live, with applications to object recognition in natural images, object detection, and obstacle avoidance for mobile robots.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create df and convert to csv"
      ],
      "metadata": {
        "id": "nKNl4pKw44lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(parsed_entries, columns=[\"year\", \"author(s)\", \"affiliation(s)\", \"title\", \"type\", \"abstract\"])"
      ],
      "metadata": {
        "id": "5IEUDPyalsYH"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "AZRpkwpEJ6ut",
        "outputId": "ddc5558e-7446-4a6b-9eef-9ce9266c0fa6"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   year                        author(s)  \\\n",
              "0  2011      Barbara Anne Dosher, Irvine   \n",
              "1  2011                       Yann LeCun   \n",
              "2  2011                  Janne V. Kujala   \n",
              "3  2011  F. Gregory Ashby, Santa Barbara   \n",
              "4  2011                     Noah Silbert   \n",
              "\n",
              "                           affiliation(s)  \\\n",
              "0                University of California   \n",
              "1  Courant Institute; New York University   \n",
              "2                 University of Jyvaskyla   \n",
              "3                University of California   \n",
              "4                  University of Maryland   \n",
              "\n",
              "                                               title type  \\\n",
              "0  Changing the State of the Observer in Visual P...        \n",
              "1  Training Hierarchical Systems for Visual Perce...        \n",
              "2  Dependence of random variables on external fac...        \n",
              "3      A Brief History of General Recognition Theory        \n",
              "4  Two hierarchical Bayesian General Recognition ...        \n",
              "\n",
              "                                            abstract  \n",
              "0  Processing of visual inputs is fundamental to ...  \n",
              "1  Over the last few years, a surprising converge...  \n",
              "2  In this talk I will address two distinct topic...  \n",
              "3  General recognition theory (GRT) was initially...  \n",
              "4  General Recognition Theory (GRT) provides a po...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8b711207-59f6-4790-984e-8b626ff7a876\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>author(s)</th>\n",
              "      <th>affiliation(s)</th>\n",
              "      <th>title</th>\n",
              "      <th>type</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2011</td>\n",
              "      <td>Barbara Anne Dosher, Irvine</td>\n",
              "      <td>University of California</td>\n",
              "      <td>Changing the State of the Observer in Visual P...</td>\n",
              "      <td></td>\n",
              "      <td>Processing of visual inputs is fundamental to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011</td>\n",
              "      <td>Yann LeCun</td>\n",
              "      <td>Courant Institute; New York University</td>\n",
              "      <td>Training Hierarchical Systems for Visual Perce...</td>\n",
              "      <td></td>\n",
              "      <td>Over the last few years, a surprising converge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2011</td>\n",
              "      <td>Janne V. Kujala</td>\n",
              "      <td>University of Jyvaskyla</td>\n",
              "      <td>Dependence of random variables on external fac...</td>\n",
              "      <td></td>\n",
              "      <td>In this talk I will address two distinct topic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2011</td>\n",
              "      <td>F. Gregory Ashby, Santa Barbara</td>\n",
              "      <td>University of California</td>\n",
              "      <td>A Brief History of General Recognition Theory</td>\n",
              "      <td></td>\n",
              "      <td>General recognition theory (GRT) was initially...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2011</td>\n",
              "      <td>Noah Silbert</td>\n",
              "      <td>University of Maryland</td>\n",
              "      <td>Two hierarchical Bayesian General Recognition ...</td>\n",
              "      <td></td>\n",
              "      <td>General Recognition Theory (GRT) provides a po...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b711207-59f6-4790-984e-8b626ff7a876')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8b711207-59f6-4790-984e-8b626ff7a876 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8b711207-59f6-4790-984e-8b626ff7a876');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3622a68c-c027-4c98-9a53-4ee9220140df\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3622a68c-c027-4c98-9a53-4ee9220140df')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3622a68c-c027-4c98-9a53-4ee9220140df button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 143,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2011\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author(s)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 140,\n        \"samples\": [\n          \"Jay Verkuilen, Clintin Davis-Stober, Nicholas Brown\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"affiliation(s)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 109,\n        \"samples\": [\n          \"Indiana University; Indiana University; Indiana University; Indiana University\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 143,\n        \"samples\": [\n          \"The Development Of Context Use And Three Way Bindings In Episodic Memory\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 142,\n        \"samples\": [\n          \"We present a package for the analysis of (nonhierarchical) Multinomial Processing Tree (MPT) models for the statistical programming language R: MPTinR. MPTinR combines two approaches, extending the functionality of other software for MPTs: (1) ease of use and (2) amount of functionalities. Regarding (1), MPTinR offers seamless integration with R, with both model and (sequential equality and sequential inequality) restrictions being conveniently defined in external files (also compatible with previous software implementations; e.g., multiTree). Regarding (2), MPTinR automatically provides fits for singleand multi-individual data (optionally) applying both equality and inequality restrictions. Model selection can be achieved with AIC, BIC and, using the minimum description length based Fisher Information Approximation (FIA; using the algorithm by Wu, Myung & Batchelder, 2010, ported to R). FIA can be conveniently computed by means of a simple model file given that the model\\u2019s context-free representation (Purdy & Batchelder, 2009) is automatically constructed from the equations. Furthermore, MPTinR offers data generation and (parametric and nonparametric) bootstrap functionalities for model inference. Several functionalities can be accelerated using multiple cores. The homepage of MPTinR (including download link and documentation) is: http://www.psychologie.unifreiburg.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(f\"/content/drive/MyDrive/math_psych_work/csv/smp{year}_program.csv\", index=False)"
      ],
      "metadata": {
        "id": "65YB5d8WESQZ"
      },
      "execution_count": 231,
      "outputs": []
    }
  ]
}