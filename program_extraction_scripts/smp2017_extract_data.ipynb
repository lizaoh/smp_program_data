{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP/pTS60BGES5UMSCnIME4V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizaoh/smp_program_data/blob/main/smp2017_extract_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top of Script"
      ],
      "metadata": {
        "id": "KBXc6uieLC6Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW0MLlXyhw5G",
        "outputId": "75c3224a-69dd-4cdb-8694-618b935644e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "!pip install pymupdf-layout\n",
        "!pip install pymupdf4llm\n",
        "!pip install wordfreq\n",
        "# !pip install rapidfuzz\n",
        "import glob\n",
        "import os\n",
        "import pathlib\n",
        "import pymupdf\n",
        "import pymupdf.layout\n",
        "import pymupdf4llm\n",
        "import re\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import wordfreq\n",
        "# from rapidfuzz import process, fuzz"
      ],
      "metadata": {
        "id": "iwaO2sqpib2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492e7f44-6e5c-4b34-9d19-8dc43d1b42e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.7\n",
            "Collecting pymupdf-layout\n",
            "  Downloading pymupdf_layout-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting PyMuPDF==1.26.6 (from pymupdf-layout)\n",
            "  Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from pymupdf-layout) (6.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pymupdf-layout) (2.0.2)\n",
            "Collecting onnxruntime (from pymupdf-layout)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pymupdf-layout) (3.6.1)\n",
            "Collecting coloredlogs (from onnxruntime->pymupdf-layout)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (1.14.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->pymupdf-layout)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime->pymupdf-layout) (1.3.0)\n",
            "Downloading pymupdf_layout-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF, humanfriendly, coloredlogs, onnxruntime, pymupdf-layout\n",
            "  Attempting uninstall: PyMuPDF\n",
            "    Found existing installation: PyMuPDF 1.26.7\n",
            "    Uninstalling PyMuPDF-1.26.7:\n",
            "      Successfully uninstalled PyMuPDF-1.26.7\n",
            "Successfully installed PyMuPDF-1.26.6 coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.2 pymupdf-layout-1.26.6\n",
            "Collecting pymupdf4llm\n",
            "  Downloading pymupdf4llm-0.2.8-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pymupdf>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from pymupdf4llm) (1.26.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from pymupdf4llm) (0.9.0)\n",
            "Downloading pymupdf4llm-0.2.8-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf4llm\n",
            "Successfully installed pymupdf4llm-0.2.8\n",
            "Collecting wordfreq\n",
            "  Downloading wordfreq-3.1.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ftfy>=6.1 (from wordfreq)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting langcodes>=3.0 (from wordfreq)\n",
            "  Downloading langcodes-3.5.1-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
            "  Downloading locate-1.1.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from wordfreq) (1.1.2)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.12/dist-packages (from wordfreq) (2025.11.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy>=6.1->wordfreq) (0.2.14)\n",
            "Downloading wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langcodes-3.5.1-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.1/183.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
            "Installing collected packages: locate, langcodes, ftfy, wordfreq\n",
            "Successfully installed ftfy-6.3.1 langcodes-3.5.1 locate-1.1.1 wordfreq-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdfs_path = '/content/drive/MyDrive/math_psych_work/Conference Programs/'"
      ],
      "metadata": {
        "id": "9TqNj0OYhzgb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions\n",
        "Created with help from GPT 5.2, but some are my own code just turned into a function."
      ],
      "metadata": {
        "id": "cK6Itu79JGuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOCATIONS = [\n",
        "    'United States of America', 'United States', 'Switzerland', 'Japan', 'Bremen',\n",
        "    'Berlin, Germany', 'Heidelberg, Germany', 'Germany', 'Berlin', 'Norway',\n",
        "    'Turkey', 'Belgium', 'Italy', 'Israel', 'New Brunswick, New Jersey',\n",
        "    'Australia', 'The Netherlands', 'USA', 'Netherlands, The', 'Netherlands',\n",
        "    'United Kingdom', 'Singapore', 'France', 'Dayton OH', 'Dayton', 'India',\n",
        "    'Taiwan, Republic of China', 'Austria', 'Canada', 'Denmark', 'Spain',\n",
        "    'Edmonton', 'Bloomington, Indiana', 'Indiana', 'Russian Federation',\n",
        "    'University Park, Pennsylvania', 'California', 'San Francisco, California',\n",
        "    'Taipei, Taiwan', 'Charlottesville, Virginia', 'New York, New York',\n",
        "    'Toronto, Ontario', 'New Haven, Connecticut', 'Ann Arbor, Michigan', 'Ohio',\n",
        "    'Ottawa, Ontario', 'Houston, Texas', 'UK', 'New Brunswick, Piscataway, NJ',\n",
        "    'Finland'\n",
        "]\n",
        "\n",
        "# compile once\n",
        "LOCATION_RE = re.compile(\n",
        "    r',\\s*(?:' + '|'.join(map(re.escape, LOCATIONS)) + r')\\b',\n",
        "    re.I\n",
        ")\n",
        "\n",
        "def remove_locations(entry: str) -> str:\n",
        "    return LOCATION_RE.sub('', entry).strip()"
      ],
      "metadata": {
        "id": "xXzXB83HNC3z"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authors here are written normally so no comma\n",
        "AUTHOR_LINE_RE = re.compile(\n",
        "    r\"\"\"\n",
        "    ^\n",
        "    # First author\n",
        "    (?:[a-z]{2,}\\s+)*[A-Z][a-zA-Z'-]+,      # last name (with lowercase particles)\n",
        "    \\s+\n",
        "    (?:[A-Z][a-zA-Z'-]+|[A-Z]\\.)            # first name OR initial\n",
        "    (?:\\s+(?:[A-Z][a-zA-Z'-]+|[A-Z]\\.))*    # middle names / initials\n",
        "\n",
        "    # Additional authors\n",
        "    (?:,\\s+\n",
        "        (?:[a-z]{2,}\\s+)*[A-Z][a-zA-Z'-]+,\n",
        "        \\s+\n",
        "        (?:[A-Z][a-zA-Z'-]+|[A-Z]\\.)\n",
        "        (?:\\s+(?:[A-Z][a-zA-Z'-]+|[A-Z]\\.))*\n",
        "    )*\n",
        "\n",
        "    # Optional \", and Last, First\"\n",
        "    (?:,\\s+and\\s+\n",
        "        (?:[a-z]{2,}\\s+)*[A-Z][a-zA-Z'-]+,\n",
        "        \\s+\n",
        "        (?:[A-Z][a-zA-Z'-]+|[A-Z]\\.)\n",
        "        (?:\\s+(?:[A-Z][a-zA-Z'-]+|[A-Z]\\.))*\n",
        "    )?\n",
        "\n",
        "    \\.?\n",
        "    $\n",
        "    \"\"\",\n",
        "    re.VERBOSE\n",
        ")\n",
        "\n",
        "def is_author_only_line(line: str) -> bool:\n",
        "    return bool(AUTHOR_LINE_RE.match(line.strip()))"
      ],
      "metadata": {
        "id": "d_W6lTIjSMLE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_authors(text: str) -> str:\n",
        "    if not text:\n",
        "        return text\n",
        "\n",
        "    TEXT_TO_DELETE = [\n",
        "        r'\\[\\d{2}/\\d{2}\\s+\\d{2}:\\d{2}\\]\\s*Tiered',  # [24/07 11:20] Tiered\n",
        "        r'##',\n",
        "        r'\\bThe Slate\\b',\n",
        "        r'Space\\s+\\d{2}\\s+Scarman',\n",
        "        r'Tiered\\s+Scarman',\n",
        "    ]\n",
        "\n",
        "    for pattern in TEXT_TO_DELETE:\n",
        "        text = re.sub(pattern, '', text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "BOv3czWobwDm"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_authors_with_indices(auth_text: str):\n",
        "    authors = []\n",
        "\n",
        "    for name, nums in re.findall(r'([^,\\[]+)\\[([0-9,]+)\\]', auth_text):\n",
        "        indices = [int(n) for n in nums.split(',')]\n",
        "        authors.append((name.strip().lstrip('and '), indices))\n",
        "\n",
        "    return authors"
      ],
      "metadata": {
        "id": "aNBAyT93ZbiB"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_affiliation_numbers(text: str) -> str:\n",
        "    # Turn [3] → 3\n",
        "    return re.sub(r'\\[(\\d+)\\]', r'\\1', text)"
      ],
      "metadata": {
        "id": "Tx3GQnqNp9EY"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_affiliation_dict(aff_text: str) -> dict[int, str]:\n",
        "    aff_text = normalize_affiliation_numbers(aff_text)\n",
        "\n",
        "    out = {}\n",
        "\n",
        "    # Pattern A: number FIRST\n",
        "    for m in re.finditer(r'(\\d+)\\s+([^0-9]+?)(?=(?:\\s*\\d+\\s+)|$)', aff_text):\n",
        "        num = int(m.group(1))\n",
        "        aff = m.group(2).strip(' ,')\n",
        "        out[num] = aff\n",
        "\n",
        "    # Pattern B: number LAST\n",
        "    for m in re.finditer(r'([^0-9]+?)\\s*(\\d+)(?=\\s*(?:,|$))', aff_text):\n",
        "        num = int(m.group(2))\n",
        "        aff = m.group(1).strip(' ,')\n",
        "        out[num] = aff\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "W1pzD77VbUy2"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_aff_list(authors, aff_dict):\n",
        "    author_names = []\n",
        "    author_affiliations = []\n",
        "\n",
        "    for name, indices in authors:\n",
        "        author_names.append(name)\n",
        "\n",
        "        affs = [\n",
        "            aff_dict[i]\n",
        "            for i in indices\n",
        "            if i in aff_dict\n",
        "        ]\n",
        "\n",
        "        # join multiple affiliations for the SAME author with \" and \"\n",
        "        author_affiliations.append(\" / \".join(affs))\n",
        "\n",
        "    new_authors = \", \".join(author_names)\n",
        "    new_affiliations = \"; \".join(author_affiliations)\n",
        "\n",
        "    return new_authors, new_affiliations"
      ],
      "metadata": {
        "id": "R8Aq-wH9W_8r"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rehyphenate_words(text, words_to_hyphenate):\n",
        "    for word, hyphenated_word in words_to_hyphenate:\n",
        "        text = text.replace(word, hyphenated_word)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "npwIxgb9Yo2U"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_trailing_text(text):\n",
        "  no_trailing_junk = text.split(\".\")[:-1]\n",
        "\n",
        "  return \".\".join(no_trailing_junk) + \".\""
      ],
      "metadata": {
        "id": "GGh2K2HPGXMF"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_schedule_noise(text: str) -> str:\n",
        "    if not text:\n",
        "        return text\n",
        "\n",
        "    # 1. Remove date/time/location lines\n",
        "    text = re.sub(\n",
        "        r'''\n",
        "        \\n*                             # leading newlines\n",
        "        \\d{2}/\\d{2}                    # date\n",
        "        \\s+\\d{2}:\\d{2}                 # time\n",
        "        [^\\n]*                         # rest of line (location)\n",
        "        \\n*                            # trailing newlines\n",
        "        ''',\n",
        "        '\\n\\n',\n",
        "        text,\n",
        "        flags=re.VERBOSE\n",
        "    )\n",
        "\n",
        "    # 2. Remove standalone page numbers (e.g. \"3\", \"129\")\n",
        "    text = re.sub(\n",
        "        r'\\n\\s*\\d{1,3}\\s*\\n',\n",
        "        '\\n\\n',\n",
        "        text\n",
        "    )\n",
        "\n",
        "    # 3. Normalize whitespace\n",
        "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "CxB8mawESnT_"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_title_noise(text: str) -> str:\n",
        "  # Remove text interrupting the title\n",
        "  text = re.sub(\n",
        "    r'\\*\\*\\s+- \\d{2}/\\d{2} \\d{2}:\\d{2}\\s+\\*\\*',\n",
        "    ' ',\n",
        "    text)\n",
        "\n",
        "  text = re.sub(\n",
        "      r'\\*\\* \\d{2}/\\d{2} \\d{2}:\\d{2} \\*\\*',\n",
        "      ' ',\n",
        "      text\n",
        "      )\n",
        "\n",
        "  abstracts = text.replace('** \\n\\n## **', ' ')\n",
        "\n",
        "  return abstracts"
      ],
      "metadata": {
        "id": "Jlh3PFqiHbM7"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text, words_to_hyphenate=None):\n",
        "    if not text:\n",
        "        return text\n",
        "\n",
        "    if words_to_hyphenate:\n",
        "      text = rehyphenate_words(text, words_to_hyphenate)\n",
        "\n",
        "    text = remove_schedule_noise(text)\n",
        "    text = remove_trailing_text(text)\n",
        "    text = text.replace('\\n\\n[', '[') # Remove new lines for references\n",
        "                                      # at the end of one abstract\n",
        "    text = re.sub(r'\\n\\n\\*\\*==> picture [\\d{3} x \\d{2}] intentionally omitted <==\\*\\*\\n\\n', '', text)\n",
        "    text = re.sub(r' \\n\\n\\d{1,3} \\n\\n', ' ', text)  # Remove page breaks with page number\n",
        "    text = re.sub(r'\\n\\n\\d{2}', '', text)\n",
        "\n",
        "    text = re.sub(r'-\\s+(?!\\b(?:and|or)\\b)', '', text)  # Get rid of - and space after\n",
        "                                                        # unless word after is\n",
        "                                                        # \"and\" or \"or\"\n",
        "\n",
        "    text = text.strip()\n",
        "    text = fix_ligatures(text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "aq4f3JE9IgHD"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LIGATURE_MAP = {\n",
        "    \"ﬁ\": \"fi\", \"ﬂ\": \"fl\", \"ﬃ\": \"ffi\", \"ﬄ\": \"ffl\", \"ﬀ\": \"ff\", \"ﬅ\": \"ft\", \"ﬆ\": \"st\",\n",
        "    \"Æ\": \"ffi\", \"¨u\": \"ü\", \"¨a\": \"ä\", \"´e\": \"é\", \"`e\": \"è\", \"`a\": \"à\", \"¨o\": \"ö\",\n",
        "    \"˚a\": \"å\", \"c¸\": \"ç\", '“': '\"', '”': '\"', \"’\": \"'\", '˜n': 'ñ', 'ˇs': 'š',\n",
        "    \"âĂŸ\": \"'\", \"``\": '\"', \"↵\": \"ff\", \"✏\": \"ffl\"\n",
        "}\n",
        "\n",
        "def fix_ligatures(text):\n",
        "    # Replace known ligatures\n",
        "    for bad, good in LIGATURE_MAP.items():\n",
        "        text = text.replace(bad, good)\n",
        "\n",
        "    # Replace any private-use ligature (common in PDFs)\n",
        "    cleaned_chars = []\n",
        "    for ch in text:\n",
        "        name = unicodedata.name(ch, \"\")\n",
        "        if \"LIGATURE\" in name.upper():\n",
        "            # Try to break it apart: remove spaces and lowercase\n",
        "            base = name.split(\"LIGATURE\")[-1]\n",
        "            base = base.replace(\" \", \"\").lower()\n",
        "            cleaned_chars.append(base)\n",
        "        else:\n",
        "            cleaned_chars.append(ch)\n",
        "\n",
        "    return \"\".join(cleaned_chars)"
      ],
      "metadata": {
        "id": "WLDsGBzsFnSz"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checks if valid word using Zipf frequency\n",
        "def is_probably_valid(word, threshold=2.5):\n",
        "    return wordfreq.zipf_frequency(word, \"en\") > threshold  # smaller number cuts off\n",
        "                                                            # more words, bigger is\n",
        "                                                            # more lenient"
      ],
      "metadata": {
        "id": "7DrUdRHGgTrp"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Program\n",
        "\n",
        "241 entries total (3 plenary talks, 25 symposium talks, 140 talks, and 73 posters)\n",
        "\n",
        "There are affiliations listed again here.\n"
      ],
      "metadata": {
        "id": "EzyoCbvJK6js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grab text from the pdf"
      ],
      "metadata": {
        "id": "NY0NZXAh696-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "year = '2017'\n",
        "program = pymupdf.open(pdfs_path + f'smp{year}_program.pdf')"
      ],
      "metadata": {
        "id": "-iaeTkOjqA3-"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "program_text = pymupdf4llm.to_markdown(program)"
      ],
      "metadata": {
        "id": "GB3Wc1MBHp8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6fedcad-10fe-4493-b13e-17d72a86c1a3"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Document parser messages ===\n",
            "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000Full-page OCR on page.number=10/11.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "program_text[80_600:83_000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "7-jva4kegu_-",
        "outputId": "94e4346c-6f7a-4e9d-865b-0d5dc3dcf8cd"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'uthor Index**<br>**133**|\\n\\n\\n\\nxvii \\n\\nxviii \\n\\n## **Abstracts** \\n\\n## **Some things that glitter are gold: A Shiny app for an Instance-Based Learning model** \\n\\nCleotilde Gonzalez, Je↵rey Stephen Chrabaszcz Carnegie Mellon University, United States of America \\n\\nDeparting from the fact that models are only a representation of reality and not reality itself, all models are wrong. Therefore, building models that are useful and finding ways to e↵ectively communicate their emerging insights are perhaps the most important goals for cognitive modelers. Unfortunately, we are terrible at getting a scientific message across to those less familiar with our models, and we often fall trap of technical complexity. The need for increased transparency, awareness and access to the insights that cognitive models can provide, has motivated the development of tools to give hands-on experiences with cognitive models. In this talk I will present our most recent attempt to make an IBL model useful to researchers and students of behavioral science. Shiny-IBL uses the R package Shiny for generating a web application written primarily in the R language. Shiny-IBL o↵ers a complementary way to communicate the complexity of dynamics emerging from the simple IBL model of binary choice. The main insight from Shiny-IBL is that cognitive modelers should go beyond the explanation of concepts that often need technical expertise and skills, and provide hands-on experiences to demonstrate and communicate the complex insights from their models without the need of additional skills. These interactive tools could also be research tools in their own right. Researchers could use Shiny-IBL to discover a set of inputs that may produce model outputs that may be surprising aspects of human behavior, and be able to understand the reasons behind it. \\n\\n23/07 09:00 The Slate \\n\\n1 \\n\\n## 23/07 09:00 Tiered Scarman \\n\\n23/07 09:00 Space 41 Scarman \\n\\n## **A computational investigation of sources of variability in sentence comprehension difficulty in aphasia** \\n\\n## Paul M¨atzig[1] , Shravan Vasishth[1] , Felix Engelmann[2] and David Caplan[3] 1 University of Potsdam, Germany, 2 The University of Manchester, 3 Massachusetts General Hospital \\n\\nWe present a computational evaluation of three hypotheses about sources of deficit in sentence comprehension in aphasia: slowed processing, intermittent deficiency, and resource redu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split text into presentation entries"
      ],
      "metadata": {
        "id": "1QnRE7dQuaDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_abstracts = program_text.split('**Abstracts** \\n\\n##')[1] # this is where abstracts start\n",
        "# Mostly makes title one chunk again if broken up by other text\n",
        "no_title_interruptions = remove_title_noise(all_abstracts)\n",
        "split_abstracts = re.split(r'\\n\\n## \\*\\*', no_title_interruptions)\n",
        "initial_abstract_entries = [entry.strip().lstrip('**') for entry in split_abstracts if entry.strip()][:-1]\n",
        "\n",
        "# Splits up entries that may have multiple talks\n",
        "abstract_entries = [\n",
        "      part.strip()\n",
        "      for entry in initial_abstract_entries\n",
        "      for part in re.split(r'\\*\\*(?=[A-Z])', entry)\n",
        "      if part.strip()\n",
        "  ]\n",
        "abstract_entries = [entry for entry in abstract_entries]"
      ],
      "metadata": {
        "id": "0xNg5F-Vn1kF"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_entries[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWkxOy-_N2h_",
        "outputId": "884383e8-0eab-47ec-87d8-7c2e3cd30a07"
      },
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Some things that glitter are gold: A Shiny app for an Instance-Based Learning model** \\n\\nCleotilde Gonzalez, Je↵rey Stephen Chrabaszcz Carnegie Mellon University, United States of America \\n\\nDeparting from the fact that models are only a representation of reality and not reality itself, all models are wrong. Therefore, building models that are useful and finding ways to e↵ectively communicate their emerging insights are perhaps the most important goals for cognitive modelers. Unfortunately, we are terrible at getting a scientific message across to those less familiar with our models, and we often fall trap of technical complexity. The need for increased transparency, awareness and access to the insights that cognitive models can provide, has motivated the development of tools to give hands-on experiences with cognitive models. In this talk I will present our most recent attempt to make an IBL model useful to researchers and students of behavioral science. Shiny-IBL uses the R package Shiny for generating a web application written primarily in the R language. Shiny-IBL o↵ers a complementary way to communicate the complexity of dynamics emerging from the simple IBL model of binary choice. The main insight from Shiny-IBL is that cognitive modelers should go beyond the explanation of concepts that often need technical expertise and skills, and provide hands-on experiences to demonstrate and communicate the complex insights from their models without the need of additional skills. These interactive tools could also be research tools in their own right. Researchers could use Shiny-IBL to discover a set of inputs that may produce model outputs that may be surprising aspects of human behavior, and be able to understand the reasons behind it. \\n\\n23/07 09:00 The Slate \\n\\n1 \\n\\n## 23/07 09:00 Tiered Scarman \\n\\n23/07 09:00 Space 41 Scarman',\n",
              " 'A computational investigation of sources of variability in sentence comprehension difficulty in aphasia** \\n\\n## Paul M¨atzig[1] , Shravan Vasishth[1] , Felix Engelmann[2] and David Caplan[3] 1 University of Potsdam, Germany, 2 The University of Manchester, 3 Massachusetts General Hospital \\n\\nWe present a computational evaluation of three hypotheses about sources of deficit in sentence comprehension in aphasia: slowed processing, intermittent deficiency, and resource reduction. The ACT-R based Lewis and Vasishth (2005) model is used to implement these three proposals. Slowed processing is implemented as slowed default production-rule firing time; intermittent deficiency as increased random noise in activation of chunks in memory; and resource reduction as reduced goal activation. As data, we considered subject vs. object relatives whose matrix clause contained either an NP or a reflexive, presented in a self-paced listening modality to 56 individuals with aphasia (IWA) and 46 matched controls. The participants heard the sentences and carried out a picture verification task to decide on an interpretation of the sentence. These response accuracies are used to identify the best parameters (for each participant) that correspond to the three hypotheses mentioned above. We show that controls have more tightly clustered (less variable) parameter values than IWA; specifically, compared to controls, among IWA there are more individuals with low goal activations, high noise, and slow default action times. This suggests that (i) individual patients show di↵erential amounts of deficit along the three dimensions of slowed processing, intermittent deficient, and resource reduction, (ii) overall, there is evidence for all three sources of deficit playing a role, and (iii) IWA have a more variable range of parameter values than controls. In sum, this study contributes a proof of concept of a quantitative implementation of, and evidence for, these three accounts of comprehension deficits in aphasia.']"
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find words to re-hyphenate"
      ],
      "metadata": {
        "id": "kLz8RAOqBM9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = re.compile(r'([A-Za-z]+)-\\n\\s*([A-Za-z]+)')\n",
        "possible_hyphenated_words = []\n",
        "\n",
        "counter = 0\n",
        "for p, page in enumerate(program[29:]):  # these are the pages with abstracts only\n",
        "  text = fix_ligatures(page.get_text('text'))\n",
        "  matches = pattern.findall(text)\n",
        "\n",
        "  for left, right in matches:\n",
        "    word = f\"{left}{right}\"\n",
        "    hyphenated_word = f\"{left}-{right}\"\n",
        "    if not is_probably_valid(word, threshold=2.8):\n",
        "      possible_hyphenated_words.append([word, hyphenated_word])\n",
        "\n",
        "      print(f\"{counter:>3}: Page {p+2:<3} {hyphenated_word:<30} {word}\")\n",
        "      counter += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMdLe69U7GMm",
        "outputId": "899e336a-ba0e-454d-ceff-c447c8438bc5"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0: Page 3   previously-observed            previouslyobserved\n",
            "  1: Page 3   iden-tifiability               identifiability\n",
            "  2: Page 6   agreement-attraction           agreementattraction\n",
            "  3: Page 8   short-term                     shortterm\n",
            "  4: Page 11  mind-wandering                 mindwandering\n",
            "  5: Page 14  proof-of                       proofof\n",
            "  6: Page 17  individual-trial               individualtrial\n",
            "  7: Page 17  individual-trial               individualtrial\n",
            "  8: Page 17  population-level               populationlevel\n",
            "  9: Page 17  over-weighting                 overweighting\n",
            " 10: Page 19  intact-rearranged              intactrearranged\n",
            " 11: Page 19  un-derlies                     underlies\n",
            " 12: Page 23  brain-imaging                  brainimaging\n",
            " 13: Page 27  decision-making                decisionmaking\n",
            " 14: Page 27  accuracy-stressed              accuracystressed\n",
            " 15: Page 28  scale-invariant                scaleinvariant\n",
            " 16: Page 30  parame-terized                 parameterized\n",
            " 17: Page 31  asym-metrically                asymmetrically\n",
            " 18: Page 32  in-tertemporal                 intertemporal\n",
            " 19: Page 33  normalisa-tion                 normalisation\n",
            " 20: Page 34  test-retest                    testretest\n",
            " 21: Page 41  relapse-recovery               relapserecovery\n",
            " 22: Page 42  Van-dekerckhove                Vandekerckhove\n",
            " 23: Page 42  categorization-decision        categorizationdecision\n",
            " 24: Page 46  Quasi-Hyperbolic               QuasiHyperbolic\n",
            " 25: Page 46  Hy-perboloid                   Hyperboloid\n",
            " 26: Page 46  attribute-based                attributebased\n",
            " 27: Page 46  Heuris-tics                    Heuristics\n",
            " 28: Page 46  intertem-poral                 intertemporal\n",
            " 29: Page 51  cross-measure                  crossmeasure\n",
            " 30: Page 52  trade-offs                     tradeoffs\n",
            " 31: Page 53  non-dynamical                  nondynamical\n",
            " 32: Page 57  Oxygen-Dependent               OxygenDependent\n",
            " 33: Page 57  electro-magnetoencephalography electromagnetoencephalography\n",
            " 34: Page 57  Ascer-taining                  Ascertaining\n",
            " 35: Page 57  neuroimag-ing                  neuroimaging\n",
            " 36: Page 57  glu-tamatergic                 glutamatergic\n",
            " 37: Page 57  incongru-ent                   incongruent\n",
            " 38: Page 57  factorial-technology           factorialtechnology\n",
            " 39: Page 57  parame-terized                 parameterized\n",
            " 40: Page 58  open-source                    opensource\n",
            " 41: Page 60  declar-ative                   declarative\n",
            " 42: Page 63  electroencephalo-graphic       electroencephalographic\n",
            " 43: Page 66  data-driven                    datadriven\n",
            " 44: Page 66  identi-fiability               identifiability\n",
            " 45: Page 66  forward-graded                 forwardgraded\n",
            " 46: Page 67  iden-tifiability               identifiability\n",
            " 47: Page 69  IV-TR                          IVTR\n",
            " 48: Page 70  Convo-lutional                 Convolutional\n",
            " 49: Page 71  Vandek-erckhove                Vandekerckhove\n",
            " 50: Page 71  ordi-nal                       ordinal\n",
            " 51: Page 72  percep-tually                  perceptually\n",
            " 52: Page 72  risk-seeking                   riskseeking\n",
            " 53: Page 73  component-based                componentbased\n",
            " 54: Page 73  reward-related                 rewardrelated\n",
            " 55: Page 76  osten-sion                     ostension\n",
            " 56: Page 76  associa-tive                   associative\n",
            " 57: Page 78  Gaus-sians                     Gaussians\n",
            " 58: Page 78  distri-butional                distributional\n",
            " 59: Page 80  multi-level                    multilevel\n",
            " 60: Page 82  multi-alternative              multialternative\n",
            " 61: Page 83  frontopo-lar                   frontopolar\n",
            " 62: Page 85  anti-saccade                   antisaccade\n",
            " 63: Page 87  itera-tively                   iteratively\n",
            " 64: Page 87  item-level                     itemlevel\n",
            " 65: Page 91  real-world                     realworld\n",
            " 66: Page 97  item-wise                      itemwise\n",
            " 67: Page 100 elic-itation                   elicitation\n",
            " 68: Page 101 exem-plar                      exemplar\n",
            " 69: Page 103 Instance-Based                 InstanceBased\n",
            " 70: Page 103 dis-entangle                   disentangle\n",
            " 71: Page 103 ex-emplar                      exemplar\n",
            " 72: Page 104 exem-plar                      exemplar\n",
            " 73: Page 105 dis-junction                   disjunction\n",
            " 74: Page 108 chunk-ing                      chunking\n",
            " 75: Page 108 oscillat-ing                   oscillating\n",
            " 76: Page 109 gram-mars                      grammars\n",
            " 77: Page 113 multi-plicativity              multiplicativity\n",
            " 78: Page 113 commu-tativity                 commutativity\n",
            " 79: Page 113 al-gorithmic                   algorithmic\n",
            " 80: Page 116 stop-signal                    stopsignal\n",
            " 81: Page 117 uni-modal                      unimodal\n",
            " 82: Page 118 it-erated                      iterated\n",
            " 83: Page 118 heterogene-ity                 heterogeneity\n",
            " 84: Page 119 Logan-Cowan                    LoganCowan\n",
            " 85: Page 119 day-to                         dayto\n",
            " 86: Page 120 qualita-tively                 qualitatively\n",
            " 87: Page 120 replica-bility                 replicability\n",
            " 88: Page 124 one-shot                       oneshot\n",
            " 89: Page 125 disagree-neutral               disagreeneutral\n",
            " 90: Page 126 syn-onymy                      synonymy\n",
            " 91: Page 127 maximum-closed                 maximumclosed\n",
            " 92: Page 127 less-regular                   lessregular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick indices of words to rehyphenate\n",
        "indices = [0, (2, 10), (12, 15), 20, 21, 23, 24, 26, (29, 32), 38, 40, 43, 45, 47, (52, 54), 60, 62, (64, 66), 69, 80, 84, 85, 88, 89, 91, 92]\n",
        "words_to_hyphenate = [\n",
        "    possible_hyphenated_words[i]\n",
        "    for item in indices\n",
        "    for i in ([item] if isinstance(item, int) else range(*item))\n",
        "]"
      ],
      "metadata": {
        "id": "9SiZxnFeRsPA"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, hyphenated_word in words_to_hyphenate:\n",
        "  print(f'{word:<30} {hyphenated_word}')"
      ],
      "metadata": {
        "id": "o7piQmxhVV4-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e069f7-a95e-4bec-8f8c-bb0c923bffc0"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "previouslyobserved             previously-observed\n",
            "agreementattraction            agreement-attraction\n",
            "shortterm                      short-term\n",
            "mindwandering                  mind-wandering\n",
            "proofof                        proof-of\n",
            "individualtrial                individual-trial\n",
            "individualtrial                individual-trial\n",
            "populationlevel                population-level\n",
            "overweighting                  over-weighting\n",
            "brainimaging                   brain-imaging\n",
            "decisionmaking                 decision-making\n",
            "accuracystressed               accuracy-stressed\n",
            "testretest                     test-retest\n",
            "relapserecovery                relapse-recovery\n",
            "categorizationdecision         categorization-decision\n",
            "QuasiHyperbolic                Quasi-Hyperbolic\n",
            "attributebased                 attribute-based\n",
            "crossmeasure                   cross-measure\n",
            "tradeoffs                      trade-offs\n",
            "nondynamical                   non-dynamical\n",
            "factorialtechnology            factorial-technology\n",
            "opensource                     open-source\n",
            "datadriven                     data-driven\n",
            "forwardgraded                  forward-graded\n",
            "IVTR                           IV-TR\n",
            "riskseeking                    risk-seeking\n",
            "componentbased                 component-based\n",
            "multialternative               multi-alternative\n",
            "antisaccade                    anti-saccade\n",
            "itemlevel                      item-level\n",
            "realworld                      real-world\n",
            "InstanceBased                  Instance-Based\n",
            "stopsignal                     stop-signal\n",
            "LoganCowan                     Logan-Cowan\n",
            "dayto                          day-to\n",
            "oneshot                        one-shot\n",
            "disagreeneutral                disagree-neutral\n",
            "maximumclosed                  maximum-closed\n",
            "lessregular                    less-regular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sort authors, affiliations, title, and abstract"
      ],
      "metadata": {
        "id": "hvO659m7ufLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_entries = []\n",
        "\n",
        "for entry in abstract_entries:\n",
        "  cleaned_entry = clean_text(entry, words_to_hyphenate)\n",
        "\n",
        "  # Gets all parts of title inbetween bolding\n",
        "  title_parts = re.findall(r'\\*\\*(.*?)\\*\\*', '**' + cleaned_entry)\n",
        "  cleaned_title = ' '.join(a.strip() for a in title_parts) if title_parts else None\n",
        "\n",
        "  rest_of_entry = cleaned_entry.split('**')[1].strip()\n",
        "  # Splits into author, affiliation, abstract for most of them\n",
        "  entry_parts = rest_of_entry.split('\\n\\n', 2)\n",
        "\n",
        "  # If only 2 parts, then authors and affiliations are on the same line\n",
        "  if len(entry_parts) == 2:\n",
        "    auth_and_aff, abstract = entry_parts\n",
        "\n",
        "    # Separates authors and affiliations\n",
        "    auth_and_aff = remove_locations(auth_and_aff)\n",
        "    auth_and_aff = auth_and_aff.replace('## ', '')\n",
        "    # If multiple affiliations\n",
        "    if ']' in auth_and_aff:\n",
        "      authors, affiliations = re.split(r'(?<=\\])\\s+(?=1)', auth_and_aff, maxsplit=1)\n",
        "    # If just one affiliation at the end, I'll fix manually\n",
        "    else:\n",
        "      authors = auth_and_aff\n",
        "      affiliations = ''\n",
        "\n",
        "  # Otherwise sort title, authors, affiliations, and abstract\n",
        "  elif len(entry_parts) == 3:\n",
        "    authors, affiliations, abstract = entry_parts\n",
        "\n",
        "\n",
        "  authors = authors.replace(' and ', ' , ')\n",
        "  # Turns affiliations into list of author's affiliations instead of just list of\n",
        "  # possible affiliations (like in previous years' programs)\n",
        "  if ']' in authors:\n",
        "    parsed_authors = parse_authors_with_indices(authors)\n",
        "    aff_dict = parse_affiliation_dict(affiliations)\n",
        "    authors, affiliations = make_aff_list(parsed_authors, aff_dict)\n",
        "\n",
        "  # Cleans up irrelevant text and extra spaces\n",
        "  cleaned_authors = clean_authors(authors)\n",
        "\n",
        "  cleaned_affiliations = remove_locations(affiliations)   # Gets rid of locations\n",
        "                                                          # of affiliations\n",
        "  cleaned_affiliations = cleaned_affiliations.strip().lstrip('## ')\n",
        "\n",
        "  cleaned_abstract = re.sub(r'\\s+', ' ', abstract.strip())\n",
        "  cleaned_abstract = remove_trailing_text(cleaned_abstract)\n",
        "\n",
        "  parsed_entries.append({\n",
        "    'year': year,\n",
        "    'author(s)': cleaned_authors,\n",
        "    'affiliation(s)': cleaned_affiliations,\n",
        "    'title': cleaned_title,\n",
        "    'type': '',\n",
        "    'abstract': cleaned_abstract\n",
        "  })"
      ],
      "metadata": {
        "id": "rYSctzOMshAh"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_entries[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q6ocLO1sha_",
        "outputId": "7dd9a8b5-511a-479e-f272-8e2708767f50"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'year': '2017',\n",
              "  'author(s)': 'Cleotilde Gonzalez, Jeffrey Stephen Chrabaszcz Carnegie Mellon University',\n",
              "  'affiliation(s)': '',\n",
              "  'title': 'Some things that glitter are gold: A Shiny app for an Instance-Based Learning model',\n",
              "  'type': '',\n",
              "  'abstract': 'Departing from the fact that models are only a representation of reality and not reality itself, all models are wrong. Therefore, building models that are useful and finding ways to effectively communicate their emerging insights are perhaps the most important goals for cognitive modelers. Unfortunately, we are terrible at getting a scientific message across to those less familiar with our models, and we often fall trap of technical complexity. The need for increased transparency, awareness and access to the insights that cognitive models can provide, has motivated the development of tools to give hands-on experiences with cognitive models. In this talk I will present our most recent attempt to make an IBL model useful to researchers and students of behavioral science. Shiny-IBL uses the R package Shiny for generating a web application written primarily in the R language. Shiny-IBL offers a complementary way to communicate the complexity of dynamics emerging from the simple IBL model of binary choice. The main insight from Shiny-IBL is that cognitive modelers should go beyond the explanation of concepts that often need technical expertise and skills, and provide hands-on experiences to demonstrate and communicate the complex insights from their models without the need of additional skills. These interactive tools could also be research tools in their own right. Researchers could use Shiny-IBL to discover a set of inputs that may produce model outputs that may be surprising aspects of human behavior, and be able to understand the reasons behind it.'},\n",
              " {'year': '2017',\n",
              "  'author(s)': 'Paul Mätzig, Shravan Vasishth, Felix Engelmann, David Caplan',\n",
              "  'affiliation(s)': 'University of Potsdam; University of Potsdam; The University of Manchester; Massachusetts General Hospital',\n",
              "  'title': 'A computational investigation of sources of variability in sentence comprehension difficulty in aphasia',\n",
              "  'type': '',\n",
              "  'abstract': 'We present a computational evaluation of three hypotheses about sources of deficit in sentence comprehension in aphasia: slowed processing, intermittent deficiency, and resource reduction. The ACT-R based Lewis and Vasishth (2005) model is used to implement these three proposals. Slowed processing is implemented as slowed default production-rule firing time; intermittent deficiency as increased random noise in activation of chunks in memory; and resource reduction as reduced goal activation. As data, we considered subject vs. object relatives whose matrix clause contained either an NP or a reflexive, presented in a self-paced listening modality to 56 individuals with aphasia (IWA) and 46 matched controls. The participants heard the sentences and carried out a picture verification task to decide on an interpretation of the sentence. These response accuracies are used to identify the best parameters (for each participant) that correspond to the three hypotheses mentioned above. We show that controls have more tightly clustered (less variable) parameter values than IWA; specifically, compared to controls, among IWA there are more individuals with low goal activations, high noise, and slow default action times. This suggests that (i) individual patients show differential amounts of deficit along the three dimensions of slowed processing, intermittent deficient, and resource reduction, (ii) overall, there is evidence for all three sources of deficit playing a role, and (iii) IWA have a more variable range of parameter values than controls. In sum, this study contributes a proof of concept of a quantitative implementation of, and evidence for, these three accounts of comprehension deficits in aphasia.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create df and convert to csv"
      ],
      "metadata": {
        "id": "nKNl4pKw44lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(parsed_entries, columns=[\"year\", \"author(s)\", \"affiliation(s)\", \"title\", \"type\", \"abstract\"])"
      ],
      "metadata": {
        "id": "5IEUDPyalsYH"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "AZRpkwpEJ6ut",
        "outputId": "ca4e4b82-089f-49f0-a12e-20559e447973"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   year                                          author(s)  \\\n",
              "0  2017  Cleotilde Gonzalez, Jeffrey Stephen Chrabaszcz...   \n",
              "1  2017  Paul Mätzig, Shravan Vasishth, Felix Engelmann...   \n",
              "2  2017                        Roger Ratcliff, Gail McKoon   \n",
              "3  2017                      Mikhail Spektor, David Kellen   \n",
              "4  2017                               Elliot Andrew Ludvig   \n",
              "\n",
              "                                      affiliation(s)  \\\n",
              "0                                                      \n",
              "1  University of Potsdam; University of Potsdam; ...   \n",
              "2                              Ohio State University   \n",
              "3           University of Basel; Syracuse University   \n",
              "4                              University of Warwick   \n",
              "\n",
              "                                               title type  \\\n",
              "0  Some things that glitter are gold: A Shiny app...        \n",
              "1  A computational investigation of sources of va...        \n",
              "2  Modeling Decision Processes on a Continuous Scale        \n",
              "3  You can't make a silk purse of a sow's ear: On...        \n",
              "4  Modeling memory biases in decisions from exper...        \n",
              "\n",
              "                                            abstract  \n",
              "0  Departing from the fact that models are only a...  \n",
              "1  We present a computational evaluation of three...  \n",
              "2  I present a model for perceptual decision maki...  \n",
              "3  Formal modeling approaches to cognition provid...  \n",
              "4  When people make decisions based on past exper...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e65c33a0-6864-4f2f-9b37-778ad198e0aa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>author(s)</th>\n",
              "      <th>affiliation(s)</th>\n",
              "      <th>title</th>\n",
              "      <th>type</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017</td>\n",
              "      <td>Cleotilde Gonzalez, Jeffrey Stephen Chrabaszcz...</td>\n",
              "      <td></td>\n",
              "      <td>Some things that glitter are gold: A Shiny app...</td>\n",
              "      <td></td>\n",
              "      <td>Departing from the fact that models are only a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017</td>\n",
              "      <td>Paul Mätzig, Shravan Vasishth, Felix Engelmann...</td>\n",
              "      <td>University of Potsdam; University of Potsdam; ...</td>\n",
              "      <td>A computational investigation of sources of va...</td>\n",
              "      <td></td>\n",
              "      <td>We present a computational evaluation of three...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017</td>\n",
              "      <td>Roger Ratcliff, Gail McKoon</td>\n",
              "      <td>Ohio State University</td>\n",
              "      <td>Modeling Decision Processes on a Continuous Scale</td>\n",
              "      <td></td>\n",
              "      <td>I present a model for perceptual decision maki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017</td>\n",
              "      <td>Mikhail Spektor, David Kellen</td>\n",
              "      <td>University of Basel; Syracuse University</td>\n",
              "      <td>You can't make a silk purse of a sow's ear: On...</td>\n",
              "      <td></td>\n",
              "      <td>Formal modeling approaches to cognition provid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017</td>\n",
              "      <td>Elliot Andrew Ludvig</td>\n",
              "      <td>University of Warwick</td>\n",
              "      <td>Modeling memory biases in decisions from exper...</td>\n",
              "      <td></td>\n",
              "      <td>When people make decisions based on past exper...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e65c33a0-6864-4f2f-9b37-778ad198e0aa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e65c33a0-6864-4f2f-9b37-778ad198e0aa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e65c33a0-6864-4f2f-9b37-778ad198e0aa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3fa2521d-0b74-42e2-ac1c-e3e9a034a104\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3fa2521d-0b74-42e2-ac1c-e3e9a034a104')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3fa2521d-0b74-42e2-ac1c-e3e9a034a104 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 241,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2017\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author(s)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 217,\n        \"samples\": [\n          \"Emily Weichart, Brandon Turner, Per Sederberg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"affiliation(s)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 196,\n        \"samples\": [\n          \"Simon Segert, Sanghyuk Park, Clintin Davis-Stober\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 241,\n        \"samples\": [\n          \"Rapid experiential decisions as a window to paradoxical multiattribute and risky choice patterns\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 239,\n        \"samples\": [\n          \"Real-life decisions entail integrating information across different attributes. When making such multiattribute decisions humans represent the value of alternatives in a context-sensitive fashion. That is, contrary to the prescriptions of rational choice theory, the subjective value assigned to an alternative is influenced by the attribute values of other competing alternatives. Computational insights about the nature of this contextual-sensitivity are scarce, mainly because typical experimental approaches involving choices between consumer goods or gambles offer no precise control over the information flow preceding each decision. To circumvent this problem, and inspired by research in perception and visual psychophysics, I will present a paradigm that abstracts multiattribute decisions into simpler, rapid experiential decisions. I will show that classical choice paradoxes such as violations of regularity and transitivity can be obtained using this simple task. These effects are explained by a single mechanism based on heightened (selective) attention towards more valuable or salient samples of incoming information. Following this selective integration framework, risk attitudes do not reflect the non-linearities of static value functions but the dynamics of context-sensitive valuation and value accumulation. I will present experimental results that verify this hypothesis by probing risk-attitudes in rapid experiential decisions. I will close by alluding to ways that the selective integration framework can explain aspects of the description-experience gap.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(f\"/content/drive/MyDrive/math_psych_work/csv/smp{year}_program.csv\", index=False)"
      ],
      "metadata": {
        "id": "65YB5d8WESQZ"
      },
      "execution_count": 244,
      "outputs": []
    }
  ]
}