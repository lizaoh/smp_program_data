{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNOZ9bvZXHbvI1PyBtpyrdU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizaoh/smp_program_data/blob/main/smp2015_extract_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top of Script"
      ],
      "metadata": {
        "id": "KBXc6uieLC6Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW0MLlXyhw5G",
        "outputId": "f31fdbce-97a8-425b-c056-62054b6ce45f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "!pip install pymupdf-layout\n",
        "!pip install pymupdf4llm\n",
        "!pip install wordfreq\n",
        "# !pip install rapidfuzz\n",
        "import glob\n",
        "import os\n",
        "import pathlib\n",
        "import pymupdf\n",
        "import pymupdf.layout\n",
        "import pymupdf4llm\n",
        "import re\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import wordfreq\n",
        "# from rapidfuzz import process, fuzz"
      ],
      "metadata": {
        "id": "iwaO2sqpib2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa096c77-1456-4219-b924-712491ca5917"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.7\n",
            "Collecting pymupdf-layout\n",
            "  Downloading pymupdf_layout-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting PyMuPDF==1.26.6 (from pymupdf-layout)\n",
            "  Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from pymupdf-layout) (6.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pymupdf-layout) (2.0.2)\n",
            "Collecting onnxruntime (from pymupdf-layout)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pymupdf-layout) (3.6.1)\n",
            "Collecting coloredlogs (from onnxruntime->pymupdf-layout)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (1.14.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->pymupdf-layout)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime->pymupdf-layout) (1.3.0)\n",
            "Downloading pymupdf_layout-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m157.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF, humanfriendly, coloredlogs, onnxruntime, pymupdf-layout\n",
            "  Attempting uninstall: PyMuPDF\n",
            "    Found existing installation: PyMuPDF 1.26.7\n",
            "    Uninstalling PyMuPDF-1.26.7:\n",
            "      Successfully uninstalled PyMuPDF-1.26.7\n",
            "Successfully installed PyMuPDF-1.26.6 coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.2 pymupdf-layout-1.26.6\n",
            "Collecting pymupdf4llm\n",
            "  Downloading pymupdf4llm-0.2.8-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pymupdf>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from pymupdf4llm) (1.26.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from pymupdf4llm) (0.9.0)\n",
            "Downloading pymupdf4llm-0.2.8-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf4llm\n",
            "Successfully installed pymupdf4llm-0.2.8\n",
            "Collecting wordfreq\n",
            "  Downloading wordfreq-3.1.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ftfy>=6.1 (from wordfreq)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting langcodes>=3.0 (from wordfreq)\n",
            "  Downloading langcodes-3.5.1-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
            "  Downloading locate-1.1.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from wordfreq) (1.1.2)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.12/dist-packages (from wordfreq) (2025.11.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy>=6.1->wordfreq) (0.2.14)\n",
            "Downloading wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langcodes-3.5.1-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.1/183.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
            "Installing collected packages: locate, langcodes, ftfy, wordfreq\n",
            "Successfully installed ftfy-6.3.1 langcodes-3.5.1 locate-1.1.1 wordfreq-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdfs_path = '/content/drive/MyDrive/math_psych_work/Conference Programs/'"
      ],
      "metadata": {
        "id": "9TqNj0OYhzgb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions\n",
        "Created with help from GPT 5.2, but some are my own code just turned into a function."
      ],
      "metadata": {
        "id": "cK6Itu79JGuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Authors are always [last name], [first name, sometimes middle initial] and\n",
        "# separated by commas, with last author having \", and\" before it\n",
        "AUTHOR_LINE_RE = re.compile(\n",
        "    r\"\"\"\n",
        "    ^\n",
        "    # First author\n",
        "    (?:[a-z]{2,}\\s+)*[A-Z][a-zA-Z'-]+,      # last name (with lowercase particles)\n",
        "    \\s+\n",
        "    (?:[A-Z][a-zA-Z'-]+|[A-Z]\\.)            # first name OR initial\n",
        "    (?:\\s+(?:[A-Z][a-zA-Z'-]+|[A-Z]\\.))*    # middle names / initials\n",
        "\n",
        "    # Additional authors\n",
        "    (?:,\\s+\n",
        "        (?:[a-z]{2,}\\s+)*[A-Z][a-zA-Z'-]+,\n",
        "        \\s+\n",
        "        (?:[A-Z][a-zA-Z'-]+|[A-Z]\\.)\n",
        "        (?:\\s+(?:[A-Z][a-zA-Z'-]+|[A-Z]\\.))*\n",
        "    )*\n",
        "\n",
        "    # Optional \", and Last, First\"\n",
        "    (?:,\\s+and\\s+\n",
        "        (?:[a-z]{2,}\\s+)*[A-Z][a-zA-Z'-]+,\n",
        "        \\s+\n",
        "        (?:[A-Z][a-zA-Z'-]+|[A-Z]\\.)\n",
        "        (?:\\s+(?:[A-Z][a-zA-Z'-]+|[A-Z]\\.))*\n",
        "    )?\n",
        "\n",
        "    \\.?\n",
        "    $\n",
        "    \"\"\",\n",
        "    re.VERBOSE\n",
        ")\n",
        "\n",
        "def is_author_only_line(line: str) -> bool:\n",
        "    return bool(AUTHOR_LINE_RE.match(line.strip()))"
      ],
      "metadata": {
        "id": "d_W6lTIjSMLE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_authors(line: str) -> list[str]:\n",
        "    line = line.strip().rstrip('.')\n",
        "    line = re.sub(r',?\\s*and\\s*$', '', line)\n",
        "    line = line.replace(', and ', ', ')\n",
        "\n",
        "    parts = [p.strip() for p in line.split(',') if p.strip()]\n",
        "\n",
        "    if len(parts) % 2 != 0:\n",
        "        # optional: log or inspect these cases\n",
        "        parts = parts[:-1]\n",
        "\n",
        "    return [\n",
        "        f\"{parts[i+1]} {parts[i]}\"\n",
        "        for i in range(0, len(parts), 2)\n",
        "    ]"
      ],
      "metadata": {
        "id": "QSvY9U3JSjbE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rehyphenate_words(text, words_to_hyphenate):\n",
        "    for word, hyphenated_word in words_to_hyphenate:\n",
        "        text = text.replace(word, hyphenated_word)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "npwIxgb9Yo2U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text, words_to_hyphenate=None):\n",
        "    if not text:\n",
        "        return text\n",
        "\n",
        "    if words_to_hyphenate:\n",
        "      text = rehyphenate_words(text, words_to_hyphenate)\n",
        "\n",
        "    text = re.sub(r'`\"`\\s(.*?)\\s`\"`', r'\"\\1\"', text)  # Replace `\"` with \"\n",
        "                                                      # and getting rid of extra\n",
        "                                                      # space between the quotes\n",
        "                                                      # and the first and last words\n",
        "\n",
        "    text = re.sub(r' \\n\\n\\d{1,3} \\n\\n', ' ', text)  # Remove page breaks with page number\n",
        "\n",
        "    text = re.sub(r'-\\s+(?!\\b(?:and|or)\\b)', '', text)  # Get rid of - and space after\n",
        "                                                        # unless word after is\n",
        "                                                        # \"and\" or \"or\"\n",
        "\n",
        "    text = re.sub(r'\\.\\s*##.*$', '.', text,\\\n",
        "                  flags=re.DOTALL)           # Gets rid of extraneous text after\n",
        "                                             # last sentence\n",
        "    text = text.strip()\n",
        "    text = fix_ligatures(text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "aq4f3JE9IgHD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LIGATURE_MAP = {\n",
        "    \"ﬁ\": \"fi\", \"ﬂ\": \"fl\", \"ﬃ\": \"ffi\", \"ﬄ\": \"ffl\", \"ﬀ\": \"ff\", \"ﬅ\": \"ft\", \"ﬆ\": \"st\",\n",
        "    \"Æ\": \"ffi\", \"¨u\": \"ü\", \"¨a\": \"ä\", \"´e\": \"é\", \"`e\": \"è\", \"`a\": \"à\", \"¨o\": \"ö\",\n",
        "    \"˚a\": \"å\", \"c¸\": \"ç\", '“': '\"', '”': '\"', \"’\": \"'\", '˜n': 'ñ', 'ˇs': 'š',\n",
        "    \"âĂŸ\": \"'\"\n",
        "}\n",
        "\n",
        "def fix_ligatures(text):\n",
        "    # Replace known ligatures\n",
        "    for bad, good in LIGATURE_MAP.items():\n",
        "        text = text.replace(bad, good)\n",
        "\n",
        "    # Replace any private-use ligature (common in PDFs)\n",
        "    cleaned_chars = []\n",
        "    for ch in text:\n",
        "        name = unicodedata.name(ch, \"\")\n",
        "        if \"LIGATURE\" in name.upper():\n",
        "            # Try to break it apart: remove spaces and lowercase\n",
        "            base = name.split(\"LIGATURE\")[-1]\n",
        "            base = base.replace(\" \", \"\").lower()\n",
        "            cleaned_chars.append(base)\n",
        "        else:\n",
        "            cleaned_chars.append(ch)\n",
        "\n",
        "    return \"\".join(cleaned_chars)"
      ],
      "metadata": {
        "id": "WLDsGBzsFnSz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checks if valid word using Zipf frequency\n",
        "def is_probably_valid(word, threshold=2.5):\n",
        "    return wordfreq.zipf_frequency(word, \"en\") > threshold  # smaller number cuts off\n",
        "                                                            # more words, bigger is\n",
        "                                                            # more lenient"
      ],
      "metadata": {
        "id": "7DrUdRHGgTrp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Program\n",
        "\n",
        "138 entries total (4 plenary talks, 24 symposium talks, 88 talks, and 22 posters)\n",
        "\n",
        "Markdown shows bold and italic text here, but the extracted text doesn't show the authors. Author names are \"last name, first name.\" Also no affiliations listed.\n"
      ],
      "metadata": {
        "id": "EzyoCbvJK6js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grab text from the pdf"
      ],
      "metadata": {
        "id": "NY0NZXAh696-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "year = '2015'\n",
        "program = pymupdf.open(pdfs_path + f'smp{year}_program.pdf')"
      ],
      "metadata": {
        "id": "-iaeTkOjqA3-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "program_text = pymupdf4llm.to_markdown(program)"
      ],
      "metadata": {
        "id": "GB3Wc1MBHp8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2660a1e3-3a57-40d2-a3f6-a7ebd2164571"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Document parser messages ===\n",
            "Full-page OCR on page.number=0/1.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "program_text[7100:9000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "7-jva4kegu_-",
        "outputId": "67bfcc8d-6629-4294-f61a-9daf0dbc49a8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"r all accepted submissions to the Meeting. This section is automatically generated from the submission website and may not reflect recent updates, such as withdrawn contributions or changes in authorship. \\n\\nFirst appear the plenary presentations, then the symposium presentations, then the regular submitted presentations, and finally the posters. \\n\\n## **Plenary presentations** \\n\\n## **Stability and plasticity in perceptual learning and models** \\n\\n## \\n\\nPerceptual learning improves how we see visual stimuli and is the basis of visual expertise. This talk considers some predictions and tests of an integrated reweighting framework (Dosher et al., 2013) for how perceptual learning balances plasticity and stability of visual representations and accounts for broad phenomena in perceptual learning related to the role of feedback, the challenges of learning intermixed tasks, and the role of bias in situations that are non-stationary due to learning. \\n\\n## **Reckoning with uncertainty: Learning to discern and adapt to the obscure** \\n\\n## \\n\\nHow people should and do reckon with uncertainty is one of the most vexing problems in theories of choice. A number of solutions have been proposed. One is to simply assume, like Savage (1954) and Ramsey (1926) did, that for a 'rational' individual all uncertainties can be reduced to risks by replacing objective probabilities with subjective ones. However, there are at least three alternatives to this non-ecological perspective on human cognition, all of which emphasize the human mind's ability to learn as the key adaptive asset for navigating uncertainty. One is to intuit unknown probabilities from stored knowledge about the world. \\n\\n7 \\n\\nAnother is to infer unknown probabilities approximately from statistical regularities that govern real-world gambles. The third is to sample the world for information, even in a very limited way, so as to replac\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For getting author names because they don't show up in markdown one\n",
        "page_text = []\n",
        "for page in program:\n",
        "  page_text.append(page.get_text('text'))\n",
        "\n",
        "abstract_pages_text = page_text[6:]"
      ],
      "metadata": {
        "id": "m8AD5ry7OLCK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_pages_text[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "ibdhnnlXTEFm",
        "outputId": "91d0f840-4283-4716-da62-2ebd210aefe5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Abstracts\\nBelow appear the abstracts for all accepted submissions to the Meeting.\\nThis section is\\nautomatically generated from the submission website and may not reﬂect recent updates,\\nsuch as withdrawn contributions or changes in authorship.\\nFirst appear the plenary presentations, then the symposium presentations, then the reg-\\nular submitted presentations, and ﬁnally the posters.\\nPlenary presentations\\nStability and plasticity in perceptual learning and models\\nDosher, Barbara\\nPerceptual learning improves how we see visual stimuli and is the basis of visual exper-\\ntise. This talk considers some predictions and tests of an integrated reweighting framework\\n(Dosher et al., 2013) for how perceptual learning balances plasticity and stability of visual\\nrepresentations and accounts for broad phenomena in perceptual learning related to the role\\nof feedback, the challenges of learning intermixed tasks, and the role of bias in situations\\nthat are non-stationary due to learning.\\nReckoning with uncertainty: Learning to discern and adapt to the obscure\\nHertwig, Ralph\\nHow people should and do reckon with uncertainty is one of the most vexing problems in\\ntheories of choice. A number of solutions have been proposed. One is to simply assume, like\\nSavage (1954) and Ramsey (1926) did, that for a 'rational' individual all uncertainties can\\nbe reduced to risks by replacing objective probabilities with subjective ones. However, there\\nare at least three alternatives to this non-ecological perspective on human cognition, all of\\nwhich emphasize the human mind's ability to learn as the key adaptive asset for navigating\\nuncertainty. One is to intuit unknown probabilities from stored knowledge about the world.\\n7\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split text into presentation entries"
      ],
      "metadata": {
        "id": "1QnRE7dQuaDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_abstracts = program_text.split('**Plenary presentations**')[1] # this is where abstracts start\n",
        "split_abstracts = re.split(r'\\n\\n## \\*\\*', all_abstracts)\n",
        "split_abstracts = [part for entry in split_abstracts for part in entry.split('\\n\\n**')]\n",
        "abstract_entries = ['**' + entry.strip() for entry in split_abstracts if len(entry) > 200][:-1]"
      ],
      "metadata": {
        "id": "0xNg5F-Vn1kF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_entries[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWkxOy-_N2h_",
        "outputId": "ffac4d48-64e7-4e93-b2ca-c25be5d648ad"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['**Stability and plasticity in perceptual learning and models** \\n\\n## \\n\\nPerceptual learning improves how we see visual stimuli and is the basis of visual expertise. This talk considers some predictions and tests of an integrated reweighting framework (Dosher et al., 2013) for how perceptual learning balances plasticity and stability of visual representations and accounts for broad phenomena in perceptual learning related to the role of feedback, the challenges of learning intermixed tasks, and the role of bias in situations that are non-stationary due to learning.',\n",
              " \"**Reckoning with uncertainty: Learning to discern and adapt to the obscure** \\n\\n## \\n\\nHow people should and do reckon with uncertainty is one of the most vexing problems in theories of choice. A number of solutions have been proposed. One is to simply assume, like Savage (1954) and Ramsey (1926) did, that for a 'rational' individual all uncertainties can be reduced to risks by replacing objective probabilities with subjective ones. However, there are at least three alternatives to this non-ecological perspective on human cognition, all of which emphasize the human mind's ability to learn as the key adaptive asset for navigating uncertainty. One is to intuit unknown probabilities from stored knowledge about the world. \\n\\n7 \\n\\nAnother is to infer unknown probabilities approximately from statistical regularities that govern real-world gambles. The third is to sample the world for information, even in a very limited way, so as to replace uncertainty with what Knight (1921) called statistical probabilities. This talk will focus on this third way to adapt to uncertainty and report on recent developments in research on decisions from experience.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find words to re-hyphenate"
      ],
      "metadata": {
        "id": "kLz8RAOqBM9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = re.compile(r'([A-Za-z]+)-\\n\\s*([A-Za-z]+)')\n",
        "possible_hyphenated_words = []\n",
        "\n",
        "counter = 0\n",
        "for p, page in enumerate(program[6:]):  # these are the pages with abstracts only\n",
        "  text = fix_ligatures(page.get_text('text'))\n",
        "  matches = pattern.findall(text)\n",
        "\n",
        "  for left, right in matches:\n",
        "    word = f\"{left}{right}\"\n",
        "    hyphenated_word = f\"{left}-{right}\"\n",
        "    if not is_probably_valid(word, threshold=1.6):\n",
        "      possible_hyphenated_words.append([word, hyphenated_word])\n",
        "\n",
        "      print(f\"{counter:>3}: Page {p+7:<3} {hyphenated_word:<30} {word}\")\n",
        "      counter += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2D7shYjBMP8",
        "outputId": "392060ea-28be-4fc1-9aff-fe24ce6d03ca"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0: Page 12  hyper-graphs                   hypergraphs\n",
            "  1: Page 15  search-such                    searchsuch\n",
            "  2: Page 16  supercapac-ity                 supercapacity\n",
            "  3: Page 16  distribution-free              distributionfree\n",
            "  4: Page 22  hyper-graphs                   hypergraphs\n",
            "  5: Page 25  top-down                       topdown\n",
            "  6: Page 29  exemplar-based                 exemplarbased\n",
            "  7: Page 31  first-countable                firstcountable\n",
            "  8: Page 31  D-convergence                  Dconvergence\n",
            "  9: Page 32  finer-grainedness              finergrainedness\n",
            " 10: Page 36  disen-tangles                  disentangles\n",
            " 11: Page 40  two-dimensional                twodimensional\n",
            " 12: Page 40  of-measure                     ofmeasure\n",
            " 13: Page 41  similarity-and                 similarityand\n",
            " 14: Page 42  attribute-based                attributebased\n",
            " 15: Page 48  feature-based                  featurebased\n",
            " 16: Page 50  multiple-processes             multipleprocesses\n",
            " 17: Page 59  model-based                    modelbased\n",
            " 18: Page 62  Cross-validation               Crossvalidation\n",
            " 19: Page 62  Cross-fitting                  Crossfitting\n",
            " 20: Page 64  sequence-based                 sequencebased\n",
            " 21: Page 65  subcompo-nents                 subcomponents\n",
            " 22: Page 75  fixed-threshold                fixedthreshold\n",
            " 23: Page 79  multiple-choice                multiplechoice\n",
            " 24: Page 79  free-response                  freeresponse\n",
            " 25: Page 80  sub-optimalities               suboptimalities\n",
            " 26: Page 81  speed-accuracy                 speedaccuracy\n",
            " 27: Page 83  Sixty-eight                    Sixtyeight\n",
            " 28: Page 83  re-analyzed                    reanalyzed\n",
            " 29: Page 86  high-dimensional               highdimensional\n",
            " 30: Page 86  attention-related              attentionrelated\n",
            " 31: Page 87  con-nectionist                 connectionist\n",
            " 32: Page 88  primary-stage                  primarystage\n",
            " 33: Page 88  primary-stage                  primarystage\n",
            " 34: Page 91  falla-ciously                  fallaciously\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick indices of words to rehyphenate\n",
        "indices = [1, 3, (5, 9), (11, 20), (22, 24), (26, 30), 32]\n",
        "words_to_hyphenate = [\n",
        "    possible_hyphenated_words[i]\n",
        "    for item in indices\n",
        "    for i in ([item] if isinstance(item, int) else range(*item))\n",
        "]"
      ],
      "metadata": {
        "id": "9SiZxnFeRsPA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, hyphenated_word in words_to_hyphenate:\n",
        "  print(f'{word:<30} {hyphenated_word}')"
      ],
      "metadata": {
        "id": "o7piQmxhVV4-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "603a7d97-7511-470b-b3aa-54e8a6a257f2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "searchsuch                     search-such\n",
            "distributionfree               distribution-free\n",
            "topdown                        top-down\n",
            "exemplarbased                  exemplar-based\n",
            "firstcountable                 first-countable\n",
            "Dconvergence                   D-convergence\n",
            "twodimensional                 two-dimensional\n",
            "ofmeasure                      of-measure\n",
            "similarityand                  similarity-and\n",
            "attributebased                 attribute-based\n",
            "featurebased                   feature-based\n",
            "multipleprocesses              multiple-processes\n",
            "modelbased                     model-based\n",
            "Crossvalidation                Cross-validation\n",
            "Crossfitting                   Cross-fitting\n",
            "fixedthreshold                 fixed-threshold\n",
            "multiplechoice                 multiple-choice\n",
            "speedaccuracy                  speed-accuracy\n",
            "Sixtyeight                     Sixty-eight\n",
            "reanalyzed                     re-analyzed\n",
            "highdimensional                high-dimensional\n",
            "primarystage                   primary-stage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sort authors, affiliations, title, and abstract"
      ],
      "metadata": {
        "id": "hvO659m7ufLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_entries = []\n",
        "\n",
        "for entry in abstract_entries:\n",
        "  # Skip if just symposium session description\n",
        "  if \"Symposium on\" in entry:\n",
        "    continue\n",
        "  cleaned_entry = clean_text(entry, words_to_hyphenate)\n",
        "  entry_parts = cleaned_entry.split('\\n\\n', 2)\n",
        "\n",
        "  # If no new lines before second line (where authors should be),\n",
        "  # sort first part as info and second part as abstract\n",
        "  if len(entry_parts) < 3:\n",
        "    info_text = entry_parts[0]\n",
        "    abstract = entry_parts[1]\n",
        "  # Otherwise sort title, authors, abstract (no affiliations in this program)\n",
        "  else:\n",
        "    info_text = '\\n\\n'.join(entry_parts[:2])\n",
        "    abstract = entry_parts[2]\n",
        "\n",
        "  # Extracts title\n",
        "  title_parts = re.findall(r'\\*\\*(.*?)\\*\\*', info_text)\n",
        "  title = ' '.join(a.strip() for a in title_parts) if title_parts else None\n",
        "\n",
        "  # Removes title from info_text to get authors\n",
        "  authors_text = info_text\n",
        "\n",
        "  for t in title_parts:\n",
        "    authors_text = authors_text.replace(f'**{t}**', '')\n",
        "\n",
        "  # Cleans up punctuation & whitespace\n",
        "  authors_text = authors_text.replace('\\n\\n##', '')\n",
        "  if authors_text.strip():\n",
        "    authors = parse_authors(authors_text.strip())\n",
        "    cleaned_authors = ', '.join(authors)\n",
        "  else:\n",
        "    cleaned_authors = ''\n",
        "\n",
        "  parsed_entries.append({\n",
        "    'year': year,\n",
        "    'author(s)': cleaned_authors,\n",
        "    'affiliation(s)': '',\n",
        "    'title': title,\n",
        "    'type': '',\n",
        "    'abstract': abstract.strip()\n",
        "  })"
      ],
      "metadata": {
        "id": "rYSctzOMshAh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_entries[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q6ocLO1sha_",
        "outputId": "a524688e-6b08-461b-82f6-ab98b85994e8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'year': '2015',\n",
              "  'author(s)': '',\n",
              "  'affiliation(s)': '',\n",
              "  'title': 'Stability and plasticity in perceptual learning and models',\n",
              "  'type': '',\n",
              "  'abstract': 'Perceptual learning improves how we see visual stimuli and is the basis of visual expertise. This talk considers some predictions and tests of an integrated reweighting framework (Dosher et al., 2013) for how perceptual learning balances plasticity and stability of visual representations and accounts for broad phenomena in perceptual learning related to the role of feedback, the challenges of learning intermixed tasks, and the role of bias in situations that are non-stationary due to learning.'},\n",
              " {'year': '2015',\n",
              "  'author(s)': '',\n",
              "  'affiliation(s)': '',\n",
              "  'title': 'Reckoning with uncertainty: Learning to discern and adapt to the obscure',\n",
              "  'type': '',\n",
              "  'abstract': \"How people should and do reckon with uncertainty is one of the most vexing problems in theories of choice. A number of solutions have been proposed. One is to simply assume, like Savage (1954) and Ramsey (1926) did, that for a 'rational' individual all uncertainties can be reduced to risks by replacing objective probabilities with subjective ones. However, there are at least three alternatives to this non-ecological perspective on human cognition, all of which emphasize the human mind's ability to learn as the key adaptive asset for navigating uncertainty. One is to intuit unknown probabilities from stored knowledge about the world. Another is to infer unknown probabilities approximately from statistical regularities that govern real-world gambles. The third is to sample the world for information, even in a very limited way, so as to replace uncertainty with what Knight (1921) called statistical probabilities. This talk will focus on this third way to adapt to uncertainty and report on recent developments in research on decisions from experience.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding authors from normal text extraction (not markdown)"
      ],
      "metadata": {
        "id": "2s0QRUR2-sTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_pages_text = [re.sub(r'\\sand\\s', ' and ', page) for page in abstract_pages_text]\n",
        "abstract_pages_text = [page.replace(',\\n', ', ') for page in abstract_pages_text]\n",
        "abstract_pages_text = [page.replace('-\\n', '') for page in abstract_pages_text]"
      ],
      "metadata": {
        "id": "Rc4Z6MrYEvwC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "authors_list = []\n",
        "for page in abstract_pages_text:\n",
        "  page_authors = []\n",
        "  for line in page.split('\\n'):\n",
        "    if is_author_only_line(line):\n",
        "      page_authors.append(line)\n",
        "  if page_authors:\n",
        "    for author in page_authors:\n",
        "      page_authors = parse_authors(author)\n",
        "      page_authors = ', '.join(page_authors)\n",
        "      authors_list.append(page_authors)\n",
        "\n",
        "# Delete the name listed for symposium descriptions\n",
        "indices_to_delete = [4, 12, 18]\n",
        "for k in sorted(indices_to_delete, reverse=True):\n",
        "    del authors_list[k]\n",
        "\n",
        "authors_list.insert(21, 'Joseph W. Houpt, Trisha Van Zandt, James T. Townsend')\n",
        "authors_list.insert(48, 'Brandon Turner, Leendert Van Maanen, Birte Forstmann')\n",
        "authors_list.insert(99, 'Luigi Lombardi, Antonio Calcagnì')\n",
        "authors_list.insert(109, 'Charles A Doan, Ronaldo Vigo')"
      ],
      "metadata": {
        "id": "BdSTL0wS-yNb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "authors_list[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Quwb7ibh_MjC",
        "outputId": "2bde8736-2c81-45db-819e-932570ca1002"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Barbara Dosher',\n",
              " 'Ralph Hertwig',\n",
              " 'Amy H Criss',\n",
              " 'Joachim Vandekerckhove',\n",
              " 'Christopher Olivola',\n",
              " 'Rick Dale, David Vinson',\n",
              " 'Michael Lee, Ravi Selker, Ravi Iyer',\n",
              " 'Zita Oravecz',\n",
              " 'Steven Piantadosi',\n",
              " 'Jun Zhang']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # For comparing authors (some were captured in markdown) and matching to titles\n",
        "# for e, entry in enumerate(parsed_entries):\n",
        "#   print(e)\n",
        "#   print(entry['title'])\n",
        "#   print(entry['author(s)'])\n",
        "#   print(authors_list[e])\n",
        "#   print('\\n')"
      ],
      "metadata": {
        "id": "1cV1LU0wbHjA"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adds author from `author_list` if no author in entry\n",
        "for e, entry in enumerate(parsed_entries):\n",
        "  if not entry['author(s)']:\n",
        "    entry['author(s)'] = authors_list[e]"
      ],
      "metadata": {
        "id": "rB1LWqGemveJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_entries[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iAuyK0ZugUE",
        "outputId": "10b80d10-be90-4f98-805e-7bc86250565f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'year': '2015',\n",
              "  'author(s)': 'Barbara Dosher',\n",
              "  'affiliation(s)': '',\n",
              "  'title': 'Stability and plasticity in perceptual learning and models',\n",
              "  'type': '',\n",
              "  'abstract': 'Perceptual learning improves how we see visual stimuli and is the basis of visual expertise. This talk considers some predictions and tests of an integrated reweighting framework (Dosher et al., 2013) for how perceptual learning balances plasticity and stability of visual representations and accounts for broad phenomena in perceptual learning related to the role of feedback, the challenges of learning intermixed tasks, and the role of bias in situations that are non-stationary due to learning.'},\n",
              " {'year': '2015',\n",
              "  'author(s)': 'Ralph Hertwig',\n",
              "  'affiliation(s)': '',\n",
              "  'title': 'Reckoning with uncertainty: Learning to discern and adapt to the obscure',\n",
              "  'type': '',\n",
              "  'abstract': \"How people should and do reckon with uncertainty is one of the most vexing problems in theories of choice. A number of solutions have been proposed. One is to simply assume, like Savage (1954) and Ramsey (1926) did, that for a 'rational' individual all uncertainties can be reduced to risks by replacing objective probabilities with subjective ones. However, there are at least three alternatives to this non-ecological perspective on human cognition, all of which emphasize the human mind's ability to learn as the key adaptive asset for navigating uncertainty. One is to intuit unknown probabilities from stored knowledge about the world. Another is to infer unknown probabilities approximately from statistical regularities that govern real-world gambles. The third is to sample the world for information, even in a very limited way, so as to replace uncertainty with what Knight (1921) called statistical probabilities. This talk will focus on this third way to adapt to uncertainty and report on recent developments in research on decisions from experience.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create df and convert to csv"
      ],
      "metadata": {
        "id": "nKNl4pKw44lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(parsed_entries, columns=[\"year\", \"author(s)\", \"affiliation(s)\", \"title\", \"type\", \"abstract\"])"
      ],
      "metadata": {
        "id": "5IEUDPyalsYH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "AZRpkwpEJ6ut",
        "outputId": "f863c375-045a-42f2-99ac-7ad9d4ac7b73"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   year               author(s) affiliation(s)  \\\n",
              "0  2015          Barbara Dosher                  \n",
              "1  2015           Ralph Hertwig                  \n",
              "2  2015             Amy H Criss                  \n",
              "3  2015  Joachim Vandekerckhove                  \n",
              "4  2015     Christopher Olivola                  \n",
              "\n",
              "                                               title type  \\\n",
              "0  Stability and plasticity in perceptual learnin...        \n",
              "1  Reckoning with uncertainty: Learning to discer...        \n",
              "2  Differentiation: A core mechanism underlying e...        \n",
              "3                   Cognitive latent variable models        \n",
              "4  Harnessing Big Data to understand life-or-deat...        \n",
              "\n",
              "                                            abstract  \n",
              "0  Perceptual learning improves how we see visual...  \n",
              "1  How people should and do reckon with uncertain...  \n",
              "2  Differentiation originated in the perceptual l...  \n",
              "3  We introduce cognitive latent variable models,...  \n",
              "4  How do people perceive and react to deadly eve...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-753a75f9-46d2-40f9-9bb2-15c07dde4e1f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>author(s)</th>\n",
              "      <th>affiliation(s)</th>\n",
              "      <th>title</th>\n",
              "      <th>type</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015</td>\n",
              "      <td>Barbara Dosher</td>\n",
              "      <td></td>\n",
              "      <td>Stability and plasticity in perceptual learnin...</td>\n",
              "      <td></td>\n",
              "      <td>Perceptual learning improves how we see visual...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015</td>\n",
              "      <td>Ralph Hertwig</td>\n",
              "      <td></td>\n",
              "      <td>Reckoning with uncertainty: Learning to discer...</td>\n",
              "      <td></td>\n",
              "      <td>How people should and do reckon with uncertain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015</td>\n",
              "      <td>Amy H Criss</td>\n",
              "      <td></td>\n",
              "      <td>Differentiation: A core mechanism underlying e...</td>\n",
              "      <td></td>\n",
              "      <td>Differentiation originated in the perceptual l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015</td>\n",
              "      <td>Joachim Vandekerckhove</td>\n",
              "      <td></td>\n",
              "      <td>Cognitive latent variable models</td>\n",
              "      <td></td>\n",
              "      <td>We introduce cognitive latent variable models,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015</td>\n",
              "      <td>Christopher Olivola</td>\n",
              "      <td></td>\n",
              "      <td>Harnessing Big Data to understand life-or-deat...</td>\n",
              "      <td></td>\n",
              "      <td>How do people perceive and react to deadly eve...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-753a75f9-46d2-40f9-9bb2-15c07dde4e1f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-753a75f9-46d2-40f9-9bb2-15c07dde4e1f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-753a75f9-46d2-40f9-9bb2-15c07dde4e1f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-992f0d83-0b4d-433e-a380-b1903375b09d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-992f0d83-0b4d-433e-a380-b1903375b09d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-992f0d83-0b4d-433e-a380-b1903375b09d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 144,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2015\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author(s)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 139,\n        \"samples\": [\n          \"Jack H. Wilson, Amy H Criss\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"affiliation(s)\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 140,\n        \"samples\": [\n          \"The linear operator model: Learning in the Naming Game\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 142,\n        \"samples\": [\n          \"Years of behavioral research suggest that human cognitive abilities consist of several major components that are manifested in the brain mostly through interactions in the frontal and parietal cortices. Interactions in parietal and frontal cortex are able to predict individual differences in intelligence and reasoning tasks, and functional networks between the frontal lobe and other brain regions are significantly correlated with intelligence. Typical functional connectivity studies in fMRI proceed in a confirmatory fashion\\u2013that is, researchers choose a theoretically motivated seed brain region and use correlations in functional activation between the seed region and other brain areas to predict task performance. The presented work details exploratory analysis of fMRI imaging data that models task scores as a function of pairwise correlations in activity between brain regions from a pre-specified set of brain regions of interest. Importantly, this allows for the inference of functional networks that can be used to classify task and predict task performance. We compare a regularized logistic regression model on task classification for 250 subjects in 9 tasks and find that both models can predict task with high accuracy. We use the model to visualize functional networks predictive of each task. We regress task performance on correlations between brain regions of interest and visualize functional networks that predict performance.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(f\"/content/drive/MyDrive/math_psych_work/csv/smp{year}_program.csv\", index=False)"
      ],
      "metadata": {
        "id": "65YB5d8WESQZ"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}