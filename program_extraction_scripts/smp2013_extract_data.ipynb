{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNAyo7Ow95MKywvyS5O8wbU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizaoh/smp_program_data/blob/main/smp2013_extract_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top of Script"
      ],
      "metadata": {
        "id": "KBXc6uieLC6Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW0MLlXyhw5G",
        "outputId": "4f6c1bee-9449-49a8-add2-a37fecd56683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "!pip install pymupdf-layout\n",
        "!pip install pymupdf4llm\n",
        "!pip install wordfreq\n",
        "# !pip install rapidfuzz\n",
        "import glob\n",
        "import os\n",
        "import pathlib\n",
        "import pymupdf\n",
        "import pymupdf.layout\n",
        "import pymupdf4llm\n",
        "import re\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import wordfreq\n",
        "# from rapidfuzz import process, fuzz"
      ],
      "metadata": {
        "id": "iwaO2sqpib2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c6b4af-dbf6-47d1-e97c-7172f98a3a5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.7\n",
            "Collecting pymupdf-layout\n",
            "  Downloading pymupdf_layout-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting PyMuPDF==1.26.6 (from pymupdf-layout)\n",
            "  Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from pymupdf-layout) (6.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pymupdf-layout) (2.0.2)\n",
            "Collecting onnxruntime (from pymupdf-layout)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pymupdf-layout) (3.6.1)\n",
            "Collecting coloredlogs (from onnxruntime->pymupdf-layout)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime->pymupdf-layout) (1.14.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->pymupdf-layout)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime->pymupdf-layout) (1.3.0)\n",
            "Downloading pymupdf_layout-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF, humanfriendly, coloredlogs, onnxruntime, pymupdf-layout\n",
            "  Attempting uninstall: PyMuPDF\n",
            "    Found existing installation: PyMuPDF 1.26.7\n",
            "    Uninstalling PyMuPDF-1.26.7:\n",
            "      Successfully uninstalled PyMuPDF-1.26.7\n",
            "Successfully installed PyMuPDF-1.26.6 coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.2 pymupdf-layout-1.26.6\n",
            "Collecting pymupdf4llm\n",
            "  Downloading pymupdf4llm-0.2.7-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pymupdf>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from pymupdf4llm) (1.26.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from pymupdf4llm) (0.9.0)\n",
            "Downloading pymupdf4llm-0.2.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf4llm\n",
            "Successfully installed pymupdf4llm-0.2.7\n",
            "Collecting wordfreq\n",
            "  Downloading wordfreq-3.1.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ftfy>=6.1 (from wordfreq)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting langcodes>=3.0 (from wordfreq)\n",
            "  Downloading langcodes-3.5.1-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
            "  Downloading locate-1.1.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from wordfreq) (1.1.2)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.12/dist-packages (from wordfreq) (2025.11.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy>=6.1->wordfreq) (0.2.14)\n",
            "Downloading wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langcodes-3.5.1-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.1/183.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
            "Installing collected packages: locate, langcodes, ftfy, wordfreq\n",
            "Successfully installed ftfy-6.3.1 langcodes-3.5.1 locate-1.1.1 wordfreq-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdfs_path = '/content/drive/MyDrive/math_psych_work/Conference Programs/'"
      ],
      "metadata": {
        "id": "9TqNj0OYhzgb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions\n",
        "Created with help from GPT 5.2, but some are my own code just turned into a function."
      ],
      "metadata": {
        "id": "cK6Itu79JGuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AFFILIATION_KEYWORDS = [\n",
        "    \"University\", \"College\", \"Department\", \"Center\", \"Institute\",\n",
        "    \"Laboratory\", \"School\", \"Hospital\", \"UC\", \"Centre\", \"Research\",\n",
        "    \"Corporation\", \"Defence\", \"Université\", \"Universite\", \"Universiy\",\n",
        "    \"Universidad\", \"Univeristy\"\n",
        "]\n",
        "AFFILIATION_KEYWORDS = re.compile(r'\\b(' + '|'.join(AFFILIATION_KEYWORDS) + r')\\b',\n",
        "                                  re.I)\n",
        "\n",
        "def looks_like_affiliation(chunk):\n",
        "    return bool(AFFILIATION_KEYWORDS.search(chunk))"
      ],
      "metadata": {
        "id": "t0fWgJSqXKPJ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_whitespace(s: str) -> str:\n",
        "    return \" \".join(s.replace(\"\\n\", \"\").split())"
      ],
      "metadata": {
        "id": "W6PGuwc6e3jZ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_affiliations(entry: str) -> str:\n",
        "    return re.sub(\n",
        "        r'(University of California)\\s*,?\\s*'\n",
        "        r'(Irvine|Davis|Berkeley|Los Angeles|San Diego|Santa Barbara|Santa Cruz|Riverside|Merced)',\n",
        "        r'\\1, \\2',\n",
        "        entry\n",
        "    )"
      ],
      "metadata": {
        "id": "ilIOFaZ8ArV9"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOCATIONS = [\n",
        "    'United States of America', 'Switzerland', 'Japan',\n",
        "    'Germany', 'Berlin', 'Belgium', 'Italy', 'Israel',\n",
        "    'Australia', 'The Netherlands', 'USA', 'Netherlands',\n",
        "    'United Kingdom', 'Singapore', 'France',\n",
        "    'Taiwan, Republic of China', 'Austria', 'Canada'\n",
        "]\n",
        "\n",
        "# compile once\n",
        "LOCATION_RE = re.compile(\n",
        "    r',\\s*(?:' + '|'.join(map(re.escape, LOCATIONS)) + r')\\b',\n",
        "    re.I\n",
        ")\n",
        "\n",
        "def remove_locations(entry: str) -> str:\n",
        "    return LOCATION_RE.sub('', entry).strip()"
      ],
      "metadata": {
        "id": "lvG2RkeJsiPV"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_authors_affiliations(entry: str) -> tuple[str, str]:\n",
        "    entry = normalize_whitespace(entry)\n",
        "    entry = normalize_affiliations(entry)\n",
        "\n",
        "    # split only on commas WITH SPACES\n",
        "    tokens = re.split(r'\\s+,\\s+(?=[A-Z])', entry)\n",
        "\n",
        "    authors = []\n",
        "    affiliations = []\n",
        "\n",
        "    for token in tokens:\n",
        "        token = token.strip()\n",
        "\n",
        "        # if token is one word and we already have an affiliation then attach\n",
        "        if len(token.split()) == 1 and affiliations and token[0].isupper():\n",
        "            affiliations[-1] = affiliations[-1] + \", \" + token\n",
        "            continue\n",
        "\n",
        "        if looks_like_affiliation(token):\n",
        "            affiliations.append(token)\n",
        "        else:\n",
        "            authors.append(token)\n",
        "\n",
        "    if len(set(affiliations)) == 1:\n",
        "        affiliations = affiliations[0]\n",
        "    else:\n",
        "      affiliations = \"; \".join(affiliations)\n",
        "\n",
        "    return (\n",
        "        \", \".join(authors),\n",
        "        affiliations\n",
        "    )"
      ],
      "metadata": {
        "id": "1DuX4taa_qAo"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_trailing_text(text):\n",
        "  no_trailing_junk = entry.split(\".\")[:-1]\n",
        "\n",
        "  return \".\".join(no_trailing_junk)"
      ],
      "metadata": {
        "id": "-SNzmnUOw5td"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text, words_to_hyphenate=None):\n",
        "    if not text:\n",
        "        return text\n",
        "\n",
        "    if words_to_hyphenate:\n",
        "        for word, hyphenated_word in words_to_hyphenate:\n",
        "            text = text.replace(word, hyphenated_word)\n",
        "\n",
        "    text = re.sub(r' \\n\\n\\d{1,3} \\n\\n', ' ', text)  # Remove page breaks with page number\n",
        "    text = re.sub(r'\\s*\\n\\s*', ' ', text)    # Replace newlines with spaces\n",
        "\n",
        "\n",
        "    text = re.sub(r'-\\s+(?!\\b(?:and|or)\\b)', '', text)  # Get rid of - and space after\n",
        "                                                        # unless word after is\n",
        "                                                        # \"and\" or \"or\"\n",
        "\n",
        "    text = re.sub(r'\\s{2}', ' ', text)       # Collapse two adjacent spaces into one\n",
        "\n",
        "    text = re.sub(r'\\.\\s*##.*$', '.', text,\\\n",
        "                  flags=re.DOTALL)           # Gets rid of extraneous text after\n",
        "                                             # last sentence\n",
        "    text = text.strip()\n",
        "    text = fix_ligatures(text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "aq4f3JE9IgHD"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LIGATURE_MAP = {\n",
        "    \"ﬁ\": \"fi\",\n",
        "    \"ﬂ\": \"fl\",\n",
        "    \"ﬃ\": \"ffi\",\n",
        "    \"ﬄ\": \"ffl\",\n",
        "    \"ﬀ\": \"ff\",\n",
        "    \"ﬅ\": \"ft\",\n",
        "    \"ﬆ\": \"st\",\n",
        "    \"Æ\": \"ffi\",\n",
        "    \"¨u\": \"ü\",\n",
        "    \"¨a\": \"ä\",\n",
        "    \"´e\": \"é\",\n",
        "    \"`e\": \"è\",\n",
        "    \"`a\": \"à\",\n",
        "    \"¨o\": \"ö\",\n",
        "    \"˚a\": \"å\",\n",
        "    \"c¸\": \"ç\"\n",
        "}\n",
        "\n",
        "def fix_ligatures(text):\n",
        "    # Replace known ligatures\n",
        "    for bad, good in LIGATURE_MAP.items():\n",
        "        text = text.replace(bad, good)\n",
        "\n",
        "    # Replace any private-use ligature (common in PDFs)\n",
        "    cleaned_chars = []\n",
        "    for ch in text:\n",
        "        name = unicodedata.name(ch, \"\")\n",
        "        if \"LIGATURE\" in name.upper():\n",
        "            # Try to break it apart: remove spaces and lowercase\n",
        "            base = name.split(\"LIGATURE\")[-1]\n",
        "            base = base.replace(\" \", \"\").lower()\n",
        "            cleaned_chars.append(base)\n",
        "        else:\n",
        "            cleaned_chars.append(ch)\n",
        "\n",
        "    return \"\".join(cleaned_chars)"
      ],
      "metadata": {
        "id": "WLDsGBzsFnSz"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checks if valid word using Zipf frequency\n",
        "def is_probably_valid(word, threshold=2.5):\n",
        "    return wordfreq.zipf_frequency(word, \"en\") > threshold  # smaller number cuts off\n",
        "                                                            # more words, bigger is\n",
        "                                                            # more lenient"
      ],
      "metadata": {
        "id": "7DrUdRHGgTrp"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Program\n",
        "\n",
        "177 entries total (2 keynote talks, 27 symposium talks, 106 talks, and 42 posters)\n",
        "\n",
        "Markdown shows bold and italic text here.\n"
      ],
      "metadata": {
        "id": "EzyoCbvJK6js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grab text from the pdf"
      ],
      "metadata": {
        "id": "NY0NZXAh696-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "year = '2013'\n",
        "program = pymupdf.open(pdfs_path + f'smp{year}_program.pdf')"
      ],
      "metadata": {
        "id": "-iaeTkOjqA3-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "program_text = pymupdf4llm.to_markdown(program)"
      ],
      "metadata": {
        "id": "GB3Wc1MBHp8X"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "program_text[16000:17500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "7-jva4kegu_-",
        "outputId": "3f56e4e3-7b8b-43c0-de45-71142b690aea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ing will be held in seminar room S13 (1st floor, building 6, Campus Griebnitzsee). \\n\\n6 \\n\\n## **Abstracts For Keynote Talks** \\n\\nChair: Hans Colonius \\n\\n## **Monday, 9:00** \\n\\n## Helmholtz \\n\\n**Machine learning methods for system identification in sensory psychology.** Felix A. Wichmann, _University of T¨ubingen, Germany_ . As a prerequisite to quantitative psychophysical models of sensory processing it is necessary to know to what extent decisions in behavioral tasks depend on specific stimulus features, the perceptual cues: Given the highdimensional input, which are the features the sensory systems base their computations on? Over the last years we have developed inverse machine learning methods for (potentially nonlinear) system identification, and have applied them to identify regions of visual saliency (Kienzle et al., 2009), to gender discrimination of human faces (Wichmann et al., 2005; Macke & Wichmann, 2010), and to the identification of auditory tones in noise (Sch¨onfelder & Wichmann, 2012; 2013). In my talk I will concentrate on how stimulus-response data can be analyzed relying on _L_ 1-regularized multiple logistic regression. This method prevents both over-fitting to noisy data and enforces sparse solutions. In simulations, “behavioral” data from a classical auditory tone-in-noise detection task were generated, and _L_ 1-regularized logistic regression precisely identified observer cues from a large set of covarying, interdependent stimulus features (a setting where '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split text into presentation entries"
      ],
      "metadata": {
        "id": "1QnRE7dQuaDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_abstracts = program_text.split('Chair: Hans Colonius')[1] # this is where abstracts start\n",
        "split_abstracts = re.split(r'\\n\\n\\*\\*', all_abstracts)\n",
        "entries = [entry.strip() for entry in split_abstracts if len(entry) > 120][:-18]"
      ],
      "metadata": {
        "id": "0xNg5F-Vn1kF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOM_KEYWORDS = ['Helmholtz', 'Bayes', 'Euler', 'Fechner', 'Lobby']\n",
        "room_re = re.compile(\n",
        "    r'\\b\\s*(?:' + '|'.join(ROOM_KEYWORDS) + r')\\s*\\*\\*\\b',\n",
        "    re.I\n",
        ")\n",
        "abstract_entries = []\n",
        "\n",
        "for entry in entries:\n",
        "  split_entry = re.split(room_re, entry)\n",
        "  abstract_entries.extend([entry.strip() for entry in split_entry if entry.strip()])\n",
        "\n",
        "abstract_entries = ['**' + entry for entry in abstract_entries if len(entry) > 50]"
      ],
      "metadata": {
        "id": "JNFxfc3vrLtz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_entries[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtGXgvugxds1",
        "outputId": "6630fa97-09b3-4c76-f1f5-29a01ddfeac2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['**Machine learning methods for system identification in sensory psychology.** Felix A. Wichmann, _University of T¨ubingen, Germany_ . As a prerequisite to quantitative psychophysical models of sensory processing it is necessary to know to what extent decisions in behavioral tasks depend on specific stimulus features, the perceptual cues: Given the highdimensional input, which are the features the sensory systems base their computations on? Over the last years we have developed inverse machine learning methods for (potentially nonlinear) system identification, and have applied them to identify regions of visual saliency (Kienzle et al., 2009), to gender discrimination of human faces (Wichmann et al., 2005; Macke & Wichmann, 2010), and to the identification of auditory tones in noise (Sch¨onfelder & Wichmann, 2012; 2013). In my talk I will concentrate on how stimulus-response data can be analyzed relying on _L_ 1-regularized multiple logistic regression. This method prevents both over-fitting to noisy data and enforces sparse solutions. In simulations, “behavioral” data from a classical auditory tone-in-noise detection task were generated, and _L_ 1-regularized logistic regression precisely identified observer cues from a large set of covarying, interdependent stimulus features (a setting where standard correla- \\n\\ntional and regression methods fail). In addition, the method succeeds for deterministic as well as probabilistic observers. The detailed decision rules of the simulated observers could be reconstructed from the estimated model weights, thus allowing predictions of responses on the basis of individual stimuli. Data from a real psychophysical experiment confirm the power of the proposed method. \\n\\nChair: Mark Steyvers \\n\\n## **Wednesday, 10:30**',\n",
              " '**A new perspective on noncompensatory decision-making: theoretic and empirical results.** Clintin P. Davis-Stober, _University of Missouri, United States of America_ . Lexicographic semiorders are mathematical structures often used to model non-compensatory decision processes (e.g., Fishburn, 1991; Tversky, 1969). A key feature of such models is that a decision maker considers the attributes of choice alternatives sequentially, preferring one choice alternative over another if, and only if, a pair of attribute values differ by a fixed threshold, i.e., a semiorder structure. I present a lexicographic semiorder model of probabilistic choice that allows a decision maker to have varying preferences with the restriction that at each sampled time point the decision maker’s preferences are consistent with a lexicographic semiorder. \\n\\n7 \\n\\nI demonstrate how this theory can be used to disentangle the response variability of a decision maker’s observed choices with the variability of his or her true preferences. When used in conjunction with existing random preference models, this theory allows for a comprehensive test of a large class of utility representations. I report the results of several new decision-making under risk experiments. We find that while traditional utility representations describe a majority of individuals, a distinct subset of decision makers consistently make choices that are best described by mixtures of lexicographic semiorders. \\n\\n8 \\n\\n## **Abstracts For Symposium Talks** \\n\\n(Symposium abstracts organized by day and presentation order) \\n\\n## **Mathematical Models of Eye Movements** \\n\\nOrganizer: Ralf Engbert \\n\\n## **Monday, 10:30** \\n\\n## Bayes']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find words to re-hyphenate"
      ],
      "metadata": {
        "id": "kLz8RAOqBM9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = re.compile(r'([A-Za-z]+)-\\n\\s*([A-Za-z]+)')\n",
        "possible_hyphenated_words = []\n",
        "\n",
        "counter = 0\n",
        "for p, page in enumerate(program[10:104]):  # these are the pages with abstracts only\n",
        "  text = fix_ligatures(page.get_text('text'))\n",
        "  matches = pattern.findall(text)\n",
        "\n",
        "  for left, right in matches:\n",
        "    word = f\"{left}{right}\"\n",
        "    hyphenated_word = f\"{left}-{right}\"\n",
        "    if not is_probably_valid(word, threshold=1.6):\n",
        "      possible_hyphenated_words.append([word, hyphenated_word])\n",
        "\n",
        "      print(f\"{counter:>3}: Page {p+7:<3} {hyphenated_word:<30} {word}\")\n",
        "      counter += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2D7shYjBMP8",
        "outputId": "63818f84-5dd6-4f40-c0cb-b5609b22cce7"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0: Page 7   high-dimensional               highdimensional\n",
            "  1: Page 7   non-compensatory               noncompensatory\n",
            "  2: Page 9   low-frequency                  lowfrequency\n",
            "  3: Page 9   fix-ational                    fixational\n",
            "  4: Page 9   Bet-tenb                       Bettenb\n",
            "  5: Page 9   Holschnei-der                  Holschneider\n",
            "  6: Page 10  Philipps-University            PhilippsUniversity\n",
            "  7: Page 11  longest-fixation               longestfixation\n",
            "  8: Page 11  Truken-brod                    Trukenbrod\n",
            "  9: Page 11  Uni-versit                     Universit\n",
            " 10: Page 12  first-order                    firstorder\n",
            " 11: Page 13  interval-ordered               intervalordered\n",
            " 12: Page 13  interval-order                 intervalorder\n",
            " 13: Page 15  speed-accuracy                 speedaccuracy\n",
            " 14: Page 15  speed-accuracy                 speedaccuracy\n",
            " 15: Page 15  Oll-man                        Ollman\n",
            " 16: Page 17  identifi-ability               identifiability\n",
            " 17: Page 19  compo-nentsacross              componentsacross\n",
            " 18: Page 19  Phil-iastides                  Philiastides\n",
            " 19: Page 22  Quasiregu-larity               Quasiregularity\n",
            " 20: Page 23  small-scale                    smallscale\n",
            " 21: Page 24  ACT-R                          ACTR\n",
            " 22: Page 24  between-subject                betweensubject\n",
            " 23: Page 24  detection-based                detectionbased\n",
            " 24: Page 25  Avra-hami                      Avrahami\n",
            " 25: Page 27  multi-armed                    multiarmed\n",
            " 26: Page 29  reference-representational     referencerepresentational\n",
            " 27: Page 30  reversal-learning              reversallearning\n",
            " 28: Page 30  two-alternative                twoalternative\n",
            " 29: Page 30  magnitude-based                magnitudebased\n",
            " 30: Page 31  N-alternative                  Nalternative\n",
            " 31: Page 32  stop-signal                    stopsignal\n",
            " 32: Page 32  Maximilians-Universit          MaximiliansUniversit\n",
            " 33: Page 35  Speck-man                      Speckman\n",
            " 34: Page 35  Psy-chonomic                   Psychonomic\n",
            " 35: Page 36  paral-lelly                    parallelly\n",
            " 36: Page 36  self-terminating               selfterminating\n",
            " 37: Page 36  Gigeren-zer                    Gigerenzer\n",
            " 38: Page 37  two-person                     twoperson\n",
            " 39: Page 37  two-alternative                twoalternative\n",
            " 40: Page 37  Bayes-optimal                  Bayesoptimal\n",
            " 41: Page 38  Speeken-brink                  Speekenbrink\n",
            " 42: Page 38  single-shot                    singleshot\n",
            " 43: Page 39  parallel-constraints           parallelconstraints\n",
            " 44: Page 40  physics-aware                  physicsaware\n",
            " 45: Page 40  city-block                     cityblock\n",
            " 46: Page 41  Fal-magne                      Falmagne\n",
            " 47: Page 45  the-art                        theart\n",
            " 48: Page 45  en-tailments                   entailments\n",
            " 49: Page 46  miss-ingness                   missingness\n",
            " 50: Page 46  Miss-BLIM                      MissBLIM\n",
            " 51: Page 46  Ste-fanutti                    Stefanutti\n",
            " 52: Page 47  guar-antying                   guarantying\n",
            " 53: Page 47  non-missing                    nonmissing\n",
            " 54: Page 47  Uni-versit                     Universit\n",
            " 55: Page 47  AND-gate                       ANDgate\n",
            " 56: Page 47  competence-based               competencebased\n",
            " 57: Page 48  Vi-dotto                       Vidotto\n",
            " 58: Page 48  iden-tifiability               identifiability\n",
            " 59: Page 49  identi-fiability               identifiability\n",
            " 60: Page 49  Humboldt-Universit             HumboldtUniversit\n",
            " 61: Page 49  Feder-meier                    Federmeier\n",
            " 62: Page 49  Cross-entropy                  Crossentropy\n",
            " 63: Page 49  repetition-induced             repetitioninduced\n",
            " 64: Page 50  task-unrelated                 taskunrelated\n",
            " 65: Page 51  Stop-Signal                    StopSignal\n",
            " 66: Page 51  classifier-accuracy            classifieraccuracy\n",
            " 67: Page 51  Humboldt-Universit             HumboldtUniversit\n",
            " 68: Page 51  magnetoencephalo-gram          magnetoencephalogram\n",
            " 69: Page 51  inter-and                      interand\n",
            " 70: Page 51  time-frequency                 timefrequency\n",
            " 71: Page 52  Humboldt-Universit             HumboldtUniversit\n",
            " 72: Page 58  self-organized                 selforganized\n",
            " 73: Page 58  com-plexifications             complexifications\n",
            " 74: Page 58  post-encoding                  postencoding\n",
            " 75: Page 59  Di-agnosticity                 Diagnosticity\n",
            " 76: Page 61  model-based                    modelbased\n",
            " 77: Page 61  Cognitive-and                  Cognitiveand\n",
            " 78: Page 62  Lud-wigsburg                   Ludwigsburg\n",
            " 79: Page 63  Uni-versit                     Universit\n",
            " 80: Page 64  mislo-cation                   mislocation\n",
            " 81: Page 64  mis-location                   mislocation\n",
            " 82: Page 65  shared-resources               sharedresources\n",
            " 83: Page 67  p-additive                     padditive\n",
            " 84: Page 68  peaked-ness                    peakedness\n",
            " 85: Page 68  human-controlled               humancontrolled\n",
            " 86: Page 68  Maximilians-University         MaximiliansUniversity\n",
            " 87: Page 70  The-Fly                        TheFly\n",
            " 88: Page 71  Teodor-escu                    Teodorescu\n",
            " 89: Page 71  post-decisional                postdecisional\n",
            " 90: Page 71  two-stage                      twostage\n",
            " 91: Page 71  Buse-meyer                     Busemeyer\n",
            " 92: Page 71  decision-confidence            decisionconfidence\n",
            " 93: Page 73  pro-variance                   provariance\n",
            " 94: Page 73  Ludwigs-Universit              LudwigsUniversit\n",
            " 95: Page 73  Ludwigs-Universit              LudwigsUniversit\n",
            " 96: Page 74  crossval-idation               crossvalidation\n",
            " 97: Page 74  non-normal                     nonnormal\n",
            " 98: Page 74  non-normally                   nonnormally\n",
            " 99: Page 74  non-normality                  nonnormality\n",
            "100: Page 75  Albert-Ludwigs                 AlbertLudwigs\n",
            "101: Page 75  Albert-Ludwigs                 AlbertLudwigs\n",
            "102: Page 77  attribute-wise                 attributewise\n",
            "103: Page 81  re-analyses                    reanalyses\n",
            "104: Page 82  commensura-bility              commensurability\n",
            "105: Page 82  Me-iCogSci                     MeiCogSci\n",
            "106: Page 83  error-patterns                 errorpatterns\n",
            "107: Page 84  well-predicted                 wellpredicted\n",
            "108: Page 84  or-thographically              orthographically\n",
            "109: Page 85  designs-per                    designsper\n",
            "110: Page 85  multiple-designs               multipledesigns\n",
            "111: Page 86  Mosha-gen                      Moshagen\n",
            "112: Page 87  Hel-versen                     Helversen\n",
            "113: Page 88  En-gbert                       Engbert\n",
            "114: Page 88  School-Age                     SchoolAge\n",
            "115: Page 88  study-test                     studytest\n",
            "116: Page 90  non-decision                   nondecision\n",
            "117: Page 90  Guessing-Based                 GuessingBased\n",
            "118: Page 90  Ludwigs-Universit              LudwigsUniversit\n",
            "119: Page 91  by-play                        byplay\n",
            "120: Page 91  two-alternative                twoalternative\n",
            "121: Page 92  continuous-state               continuousstate\n",
            "122: Page 93  approximate-inference          approximateinference\n",
            "123: Page 93  sampling-based                 samplingbased\n",
            "124: Page 95  Eng-bert                       Engbert\n",
            "125: Page 95  mi-crosaccades                 microsaccades\n",
            "126: Page 96  En-gbert                       Engbert\n",
            "127: Page 96  second-order                   secondorder\n",
            "128: Page 96  discrim-inability              discriminability\n",
            "129: Page 96  Min-imality                    Minimality\n",
            "130: Page 97  Eric-Jan                       EricJan\n",
            "131: Page 99  Univer-sit                     Universit\n",
            "132: Page 99  Univer-sit                     Universit\n",
            "133: Page 100 Bispec-trum                    Bispectrum\n",
            "134: Page 100 cross-bispectra                crossbispectra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick indices of words to rehyphenate\n",
        "indices = [(0,2), (6,7), (10,14), (20,23), (25,32), 36, (38,40), (42,45), 47, 50, (55,56), 60, (62,67), (69,72), 74, (76,77), (82,83), (85,87), (89,90), (92,95), (97,103), (106,107), (109,110), (114,123), 127, (130,131), 134]\n",
        "\n",
        "words_to_hyphenate = [\n",
        "    possible_hyphenated_words[i]\n",
        "    for item in indices\n",
        "    for i in ([item] if isinstance(item, int) else range(*item))\n",
        "]"
      ],
      "metadata": {
        "id": "9SiZxnFeRsPA"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for word, hyphenated_word in words_to_hyphenate:\n",
        "#   print(f'{word:<30} {hyphenated_word}')"
      ],
      "metadata": {
        "id": "o7piQmxhVV4-"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sort authors, affiliations, title, and abstract"
      ],
      "metadata": {
        "id": "hvO659m7ufLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_entries = []\n",
        "\n",
        "for entry in abstract_entries:\n",
        "  cleaned_entry = clean_text(entry, words_to_hyphenate)\n",
        "  if ' . ' in cleaned_entry:\n",
        "    info_text, abstract = cleaned_entry.split(' . ', 1)\n",
        "  else:\n",
        "      parsed_entries.append({\n",
        "        'year': '',\n",
        "        'author(s)': '',\n",
        "        'affiliation(s)': '',\n",
        "        'title': cleaned_entry,\n",
        "        'type': '',\n",
        "        'abstract': ''\n",
        "      })\n",
        "      continue\n",
        "\n",
        "  # Extracts title\n",
        "  title_parts = re.findall(r'\\*\\*(.*?)\\*\\*', info_text)\n",
        "  title = ' '.join(a.strip() for a in title_parts) if title_parts else None\n",
        "\n",
        "  # Extracts all affiliations in entry\n",
        "  affiliation_parts = re.findall(r'_(.*?)_', info_text)\n",
        "  affiliations = '; '.join(a.strip()\\\n",
        "                           for a in affiliation_parts)\\\n",
        "                           if affiliation_parts else None\n",
        "\n",
        "  # Removes title and affiliation from info_text to get authors\n",
        "  authors_text = info_text\n",
        "\n",
        "  for t in title_parts:\n",
        "    authors_text = authors_text.replace(f'**{t}**', '')\n",
        "\n",
        "  for a in affiliation_parts:\n",
        "    authors_text = authors_text.replace(f'_{a}_', '')\n",
        "\n",
        "  # Cleans up punctuation & whitespace\n",
        "  authors = authors_text.strip().split(',')\n",
        "  list_authors = [a.strip() for a in authors if a.strip()]\n",
        "  cleaned_authors = ', '.join(list_authors)\n",
        "\n",
        "  if len(set(affiliation_parts)) == 1:\n",
        "    affiliations = affiliation_parts[0]\n",
        "  else:\n",
        "    affiliations = '; '.join(affiliation_parts)\n",
        "\n",
        "  affiliations = remove_locations(affiliations)\n",
        "\n",
        "  parsed_entries.append({\n",
        "    'year': year,\n",
        "    'author(s)': cleaned_authors,\n",
        "    'affiliation(s)': affiliations,\n",
        "    'title': title.strip('.'),\n",
        "    'type': '',\n",
        "    'abstract': abstract\n",
        "  })"
      ],
      "metadata": {
        "id": "rYSctzOMshAh"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_entries[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q6ocLO1sha_",
        "outputId": "1077541e-8120-42ab-f35b-4fdcec7f7c5d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'year': '2013',\n",
              "  'author(s)': 'Felix A. Wichmann',\n",
              "  'affiliation(s)': 'University of Tübingen',\n",
              "  'title': 'Machine learning methods for system identification in sensory psychology',\n",
              "  'type': '',\n",
              "  'abstract': 'As a prerequisite to quantitative psychophysical models of sensory processing it is necessary to know to what extent decisions in behavioral tasks depend on specific stimulus features, the perceptual cues: Given the high-dimensional input, which are the features the sensory systems base their computations on? Over the last years we have developed inverse machine learning methods for (potentially nonlinear) system identification, and have applied them to identify regions of visual saliency (Kienzle et al., 2009), to gender discrimination of human faces (Wichmann et al., 2005; Macke & Wichmann, 2010), and to the identification of auditory tones in noise (Schönfelder & Wichmann, 2012; 2013). In my talk I will concentrate on how stimulus-response data can be analyzed relying on _L_ 1-regularized multiple logistic regression. This method prevents both over-fitting to noisy data and enforces sparse solutions. In simulations, “behavioral” data from a classical auditory tone-in-noise detection task were generated, and _L_ 1-regularized logistic regression precisely identified observer cues from a large set of covarying, interdependent stimulus features (a setting where standard correlational and regression methods fail). In addition, the method succeeds for deterministic as well as probabilistic observers. The detailed decision rules of the simulated observers could be reconstructed from the estimated model weights, thus allowing predictions of responses on the basis of individual stimuli. Data from a real psychophysical experiment confirm the power of the proposed method. Chair: Mark Steyvers ## **Wednesday, 10:30**'},\n",
              " {'year': '2013',\n",
              "  'author(s)': 'Clintin P. Davis-Stober',\n",
              "  'affiliation(s)': 'University of Missouri',\n",
              "  'title': 'A new perspective on non-compensatory decision-making: theoretic and empirical results',\n",
              "  'type': '',\n",
              "  'abstract': 'Lexicographic semiorders are mathematical structures often used to model non-compensatory decision processes (e.g., Fishburn, 1991; Tversky, 1969). A key feature of such models is that a decision maker considers the attributes of choice alternatives sequentially, preferring one choice alternative over another if, and only if, a pair of attribute values differ by a fixed threshold, i.e., a semiorder structure. I present a lexicographic semiorder model of probabilistic choice that allows a decision maker to have varying preferences with the restriction that at each sampled time point the decision maker’s preferences are consistent with a lexicographic semiorder. I demonstrate how this theory can be used to disentangle the response variability of a decision maker’s observed choices with the variability of his or her true preferences. When used in conjunction with existing random preference models, this theory allows for a comprehensive test of a large class of utility representations. I report the results of several new decision-making under risk experiments. We find that while traditional utility representations describe a majority of individuals, a distinct subset of decision makers consistently make choices that are best described by mixtures of lexicographic semiorders.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create df and convert to csv"
      ],
      "metadata": {
        "id": "nKNl4pKw44lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(parsed_entries, columns=[\"year\", \"author(s)\", \"affiliation(s)\", \"title\", \"type\", \"abstract\"])"
      ],
      "metadata": {
        "id": "5IEUDPyalsYH"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "AZRpkwpEJ6ut",
        "outputId": "065b29c2-24fc-4ea2-bd9e-30e1801f5524"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   year                                          author(s)  \\\n",
              "0  2013                                  Felix A. Wichmann   \n",
              "1  2013                            Clintin P. Davis-Stober   \n",
              "2  2013     Michele Rucci, Jonathan D. Victor, Xutao Kuang   \n",
              "3  2013  Natallia Makarava, Mario Bettenbühl, Ralf Engb...   \n",
              "4  2013        Wolfgang Einhauser¨, Bernard Marius ’t Hart   \n",
              "\n",
              "                                      affiliation(s)  \\\n",
              "0                             University of Tübingen   \n",
              "1                             University of Missouri   \n",
              "2  Boston University; Weill Cornell Medical Colle...   \n",
              "3                              University of Potsdam   \n",
              "4  Philipps-University Marburg; Center for Interd...   \n",
              "\n",
              "                                               title type  \\\n",
              "0  Machine learning methods for system identifica...        \n",
              "1  A new perspective on non-compensatory decision...        \n",
              "2  Effects of microscopic eye movements on contra...        \n",
              "3  Bayesian estimation of the scaling parameter o...        \n",
              "4  Gaze in real-world scenarios: interaction of t...        \n",
              "\n",
              "                                            abstract  \n",
              "0  As a prerequisite to quantitative psychophysic...  \n",
              "1  Lexicographic semiorders are mathematical stru...  \n",
              "2  The response characteristics of neurons in the...  \n",
              "3  In this study we re-evaluate the estimation of...  \n",
              "4  Under natural conditions gaze is a good proxy ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8eeb575-95d9-45dc-ab51-930d5d95036d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>author(s)</th>\n",
              "      <th>affiliation(s)</th>\n",
              "      <th>title</th>\n",
              "      <th>type</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2013</td>\n",
              "      <td>Felix A. Wichmann</td>\n",
              "      <td>University of Tübingen</td>\n",
              "      <td>Machine learning methods for system identifica...</td>\n",
              "      <td></td>\n",
              "      <td>As a prerequisite to quantitative psychophysic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2013</td>\n",
              "      <td>Clintin P. Davis-Stober</td>\n",
              "      <td>University of Missouri</td>\n",
              "      <td>A new perspective on non-compensatory decision...</td>\n",
              "      <td></td>\n",
              "      <td>Lexicographic semiorders are mathematical stru...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2013</td>\n",
              "      <td>Michele Rucci, Jonathan D. Victor, Xutao Kuang</td>\n",
              "      <td>Boston University; Weill Cornell Medical Colle...</td>\n",
              "      <td>Effects of microscopic eye movements on contra...</td>\n",
              "      <td></td>\n",
              "      <td>The response characteristics of neurons in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013</td>\n",
              "      <td>Natallia Makarava, Mario Bettenbühl, Ralf Engb...</td>\n",
              "      <td>University of Potsdam</td>\n",
              "      <td>Bayesian estimation of the scaling parameter o...</td>\n",
              "      <td></td>\n",
              "      <td>In this study we re-evaluate the estimation of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2013</td>\n",
              "      <td>Wolfgang Einhauser¨, Bernard Marius ’t Hart</td>\n",
              "      <td>Philipps-University Marburg; Center for Interd...</td>\n",
              "      <td>Gaze in real-world scenarios: interaction of t...</td>\n",
              "      <td></td>\n",
              "      <td>Under natural conditions gaze is a good proxy ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8eeb575-95d9-45dc-ab51-930d5d95036d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c8eeb575-95d9-45dc-ab51-930d5d95036d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c8eeb575-95d9-45dc-ab51-930d5d95036d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e76bfc48-37ce-4711-9f47-6a0450a76820\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e76bfc48-37ce-4711-9f47-6a0450a76820')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e76bfc48-37ce-4711-9f47-6a0450a76820 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 176,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"\",\n          \"2013\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author(s)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 174,\n        \"samples\": [\n          \"Martha Michalkiewicz, Alisha Coolin, Edgar Erdfelder\",\n          \"Vera G. Gryazeva-Dobshinskaya, Julia A. Dmitrieva\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"affiliation(s)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 139,\n        \"samples\": [\n          \"University of Basel; Max Planck Institute for Human Development; Vanderbilt University; Clarkson University\",\n          \"University of Padua\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 176,\n        \"samples\": [\n          \"Computational models of prediction learning: comparing decisions, reaction times, and BOLD\",\n          \"Differences between Observed and Latent Confidence in Rank Ordering\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 176,\n        \"samples\": [\n          \"Different observables, e.g., BOLD, reaction times, and decisions, can be unified by characterizing them as different windows on a common underlying computational process. In a growing approach to studying functions like reinforcement learning, data are assumed to be generated from a computational model (such as temporal difference learning) together with a set of observation models (such as hemodynamic convolution) that probabilistically link the variables computed in the model (such as reward prediction errors) to different modalities of data. This procedure converts an abstract computational model into a statistical generative model for data, whereupon standard inference techniques can be used to estimate model parameters, and also (less commonly) to compare or pool these estimates across modalities. I exemplify this approach with a series of functional neuroimaging studies of how humans learn to predict sequential events. Participants viewed a series of images generated according to a first-order Markov process, and had to identify each of them with a button press. We use a standard error-driven learning model to characterize the trial-by-trial formation of expectations about the image sequence, as reflected in the well-known facilitation of reaction times to more probable events. Next, we fit the same model to decisions in an interleaved choice task (whereby subjects effectively placed side bets about the likely image sequences) and to BOLD activity related to image expectation or decision variables in a number of areas across both tasks. By comparing estimated model parameters (specifically, the learning rate parameter, which controls how strongly the learned predictions are biased toward the most recent events) across all these datastreams, we are able to demonstrate evidence for two distinct learned representations of the task\\u2019s predictive structure. These two learning processes are associated with distinct learning rates and anatomical substrates, and combine in different ways to produce implicit (reaction time) and explicit (choice) behavior. (Joint work with Aaron Bornstein.) ## **Tuesday, 9:00** Bayes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(f\"/content/drive/MyDrive/math_psych_work/csv/smp{year}_program.csv\", index=False)"
      ],
      "metadata": {
        "id": "65YB5d8WESQZ"
      },
      "execution_count": 91,
      "outputs": []
    }
  ]
}